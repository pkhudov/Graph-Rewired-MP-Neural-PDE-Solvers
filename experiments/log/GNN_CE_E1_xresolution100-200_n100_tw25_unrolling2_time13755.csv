Training on dataset data/CE_train_E1.h5
cuda:0
models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Number of parameters: 1031645
Training started at: 2025-01-03 07:55:31
Epoch 0
Starting epoch 0...
Training Loss (progress: 0.00): 1.3951884097204317; Norm Grads: 34.64015090488698
Training Loss (progress: 0.10): 0.5443265149871516; Norm Grads: 172.0470085293121
Training Loss (progress: 0.20): 0.38620464881665145; Norm Grads: 169.66597298139612
Training Loss (progress: 0.30): 0.2919611565888543; Norm Grads: 122.77957957207234
Training Loss (progress: 0.40): 0.251262686105092; Norm Grads: 116.33243801593225
Training Loss (progress: 0.50): 0.2194574551998138; Norm Grads: 124.65596879811815
Training Loss (progress: 0.60): 0.19040681491488345; Norm Grads: 105.01752843766099
Training Loss (progress: 0.70): 0.1684728963757979; Norm Grads: 102.79061557515064
Training Loss (progress: 0.80): 0.16325526995458572; Norm Grads: 115.14973724566669
Training Loss (progress: 0.90): 0.17110326736269638; Norm Grads: 111.60774640238932
Evaluation on validation dataset:
Step 25, mean loss 0.15775641493077652
Step 50, mean loss 0.22490575324426992
Step 75, mean loss 0.2786507651213771
Step 100, mean loss 0.22589048260583022
Step 125, mean loss 0.29685277675683974
Step 150, mean loss 0.35174539045004566
Step 175, mean loss 0.6577343413572512
Step 200, mean loss 0.885395597223726
Step 225, mean loss 0.5852276311005165
Unrolled forward losses 35.33227235625826
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.13630135999072707
Step 50, mean loss 0.23676921475794568
Step 75, mean loss 0.18160745708224074
Step 100, mean loss 0.22414612733783654
Step 125, mean loss 0.3408671609819782
Step 150, mean loss 0.36662954454604857
Step 175, mean loss 0.6148965716335683
Step 200, mean loss 0.5727420228161875
Step 225, mean loss 0.5668919717844887
Unrolled forward losses 36.17324947629599
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  1:40:47.507357 

Epoch 1
Starting epoch 1...
Training Loss (progress: 0.00): 0.3296064814792551; Norm Grads: 102.18809669545573
Training Loss (progress: 0.10): 0.3199434178327626; Norm Grads: 89.95421695509606
Training Loss (progress: 0.20): 0.2880338636200419; Norm Grads: 73.05200785329464
Training Loss (progress: 0.30): 0.2800689500338722; Norm Grads: 78.17275385481399
Training Loss (progress: 0.40): 0.24080024087573856; Norm Grads: 77.15347404811132
Training Loss (progress: 0.50): 0.2549793735481371; Norm Grads: 69.11271422308965
Training Loss (progress: 0.60): 0.23178982255077718; Norm Grads: 72.07898749105016
Training Loss (progress: 0.70): 0.233168231115943; Norm Grads: 81.59260950068555
Training Loss (progress: 0.80): 0.21652250902840425; Norm Grads: 72.33590281462936
Training Loss (progress: 0.90): 0.225499463368213; Norm Grads: 74.84582194515346
Evaluation on validation dataset:
Step 25, mean loss 0.16307227048916687
Step 50, mean loss 0.14816264313516264
Step 75, mean loss 0.18483485145115858
Step 100, mean loss 0.17270827757660362
Step 125, mean loss 0.24633744433292462
Step 150, mean loss 0.22307146406715683
Step 175, mean loss 0.26363111637293246
Step 200, mean loss 0.5070849354716924
Step 225, mean loss 0.49365086250115164
Unrolled forward losses 5.698003502819951
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.1453586279273007
Step 50, mean loss 0.1329346389553134
Step 75, mean loss 0.1284870950251364
Step 100, mean loss 0.1598012862298664
Step 125, mean loss 0.19578228558312938
Step 150, mean loss 0.2546823047427818
Step 175, mean loss 0.32252070473556294
Step 200, mean loss 0.3266808269053017
Step 225, mean loss 0.34791960697469293
Unrolled forward losses 6.216960681862601
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  3:27:31.701969 

Epoch 2
Starting epoch 2...
Training Loss (progress: 0.00): 0.333402099962178; Norm Grads: 59.266213585576025
Training Loss (progress: 0.10): 0.2869425768396077; Norm Grads: 59.50298925996278
Training Loss (progress: 0.20): 0.27285824158284105; Norm Grads: 66.11294879846079
Training Loss (progress: 0.30): 0.312134004842241; Norm Grads: 74.93251033803986
Training Loss (progress: 0.40): 0.25976758024319857; Norm Grads: 71.6930507559085
Training Loss (progress: 0.50): 0.252009404083253; Norm Grads: 54.062168886221876
Training Loss (progress: 0.60): 0.2641498220665454; Norm Grads: 72.57321807016355
Training Loss (progress: 0.70): 0.2719594229310043; Norm Grads: 67.11849074370662
Training Loss (progress: 0.80): 0.286278420928472; Norm Grads: 77.39171390680549
Training Loss (progress: 0.90): 0.26295565196107; Norm Grads: 76.55202528053466
Evaluation on validation dataset:
Step 25, mean loss 0.17205564079905433
Step 50, mean loss 0.1211798216271979
Step 75, mean loss 0.1569906406389635
Step 100, mean loss 0.13519306256797428
Step 125, mean loss 0.19344248231029226
Step 150, mean loss 0.20741931466621644
Step 175, mean loss 0.21494858742178818
Step 200, mean loss 0.3212341546088131
Step 225, mean loss 0.3981558540030694
Unrolled forward losses 3.928110796156706
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.14219898904426798
Step 50, mean loss 0.10130730181730055
Step 75, mean loss 0.1134775625486474
Step 100, mean loss 0.13363720366089693
Step 125, mean loss 0.16999383691079045
Step 150, mean loss 0.2039693738676191
Step 175, mean loss 0.23741080598794978
Step 200, mean loss 0.31303157030936335
Step 225, mean loss 0.3400495031661976
Unrolled forward losses 3.991368565458914
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  5:20:04.278970 

Epoch 3
Starting epoch 3...
Training Loss (progress: 0.00): 0.2755002233438014; Norm Grads: 98.45396990557845
Training Loss (progress: 0.10): 0.28795772193406505; Norm Grads: 99.11486196794327
Training Loss (progress: 0.20): 0.22401421233268354; Norm Grads: 62.31055655414868
Training Loss (progress: 0.30): 0.23234630376879312; Norm Grads: 75.39890632868395
Training Loss (progress: 0.40): 0.23837610678476467; Norm Grads: 79.06017063242078
Training Loss (progress: 0.50): 0.2694094653506345; Norm Grads: 80.23057907560087
Training Loss (progress: 0.60): 0.2504606748610958; Norm Grads: 71.48357748906807
Training Loss (progress: 0.70): 0.26682873325485845; Norm Grads: 82.39957357545666
Training Loss (progress: 0.80): 0.20376305369495903; Norm Grads: 70.6407531659661
Training Loss (progress: 0.90): 0.24117532926118673; Norm Grads: 69.19056047958607
Evaluation on validation dataset:
Step 25, mean loss 0.13765564995005167
Step 50, mean loss 0.09840479655495114
Step 75, mean loss 0.11826052984144492
Step 100, mean loss 0.11752872413910864
Step 125, mean loss 0.16222536120993614
Step 150, mean loss 0.17665666955058232
Step 175, mean loss 0.18631982792836663
Step 200, mean loss 0.25075868225788367
Step 225, mean loss 0.34612736577963477
Unrolled forward losses 3.112773469745333
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.11435561522893722
Step 50, mean loss 0.081558607928631
Step 75, mean loss 0.09505717867455774
Step 100, mean loss 0.10561522330748836
Step 125, mean loss 0.13684158658555956
Step 150, mean loss 0.1516636778642198
Step 175, mean loss 0.19914163356171555
Step 200, mean loss 0.23343769646355045
Step 225, mean loss 0.27305084191320994
Unrolled forward losses 3.4744962647144733
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  7:17:25.917717 

Epoch 4
Starting epoch 4...
Training Loss (progress: 0.00): 0.2322143488874231; Norm Grads: 66.05772911957982
Training Loss (progress: 0.10): 0.2191201176189751; Norm Grads: 69.23015942414094
Training Loss (progress: 0.20): 0.22937893338573434; Norm Grads: 70.80080228523671
Training Loss (progress: 0.30): 0.20865815768662105; Norm Grads: 63.88444577508353
Training Loss (progress: 0.40): 0.22258571776166772; Norm Grads: 72.17925942871851
Training Loss (progress: 0.50): 0.21601398763553006; Norm Grads: 61.83279492291227
Training Loss (progress: 0.60): 0.22155746765855283; Norm Grads: 78.70028215700373
Training Loss (progress: 0.70): 0.21058154636647897; Norm Grads: 69.14291705555551
Training Loss (progress: 0.80): 0.20805614851503676; Norm Grads: 75.14902460375437
Training Loss (progress: 0.90): 0.21521872691513483; Norm Grads: 75.92019427961628
Evaluation on validation dataset:
Step 25, mean loss 0.11573148293766505
Step 50, mean loss 0.08238480859058561
Step 75, mean loss 0.098698984761394
Step 100, mean loss 0.10643392253827542
Step 125, mean loss 0.15241637466026559
Step 150, mean loss 0.16625737184328682
Step 175, mean loss 0.16647826498454088
Step 200, mean loss 0.2207548766546363
Step 225, mean loss 0.2911533754242935
Unrolled forward losses 2.9792177905368056
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.10585323480446449
Step 50, mean loss 0.07580734897669555
Step 75, mean loss 0.08208888068443855
Step 100, mean loss 0.10081370716832666
Step 125, mean loss 0.12968368556714327
Step 150, mean loss 0.1591058605169185
Step 175, mean loss 0.20389171394413796
Step 200, mean loss 0.21177544755545347
Step 225, mean loss 0.26384409467674896
Unrolled forward losses 3.3302733671628273
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  9:13:35.023899 

Epoch 5
Starting epoch 5...
Training Loss (progress: 0.00): 0.21239389992499821; Norm Grads: 55.39938535092949
Training Loss (progress: 0.10): 0.18213334163025413; Norm Grads: 68.03018815707091
Training Loss (progress: 0.20): 0.1765572973727372; Norm Grads: 58.370375642819575
Training Loss (progress: 0.30): 0.18636708342639316; Norm Grads: 59.770607494240295
Training Loss (progress: 0.40): 0.1818432026545044; Norm Grads: 62.665426176456286
Training Loss (progress: 0.50): 0.18377730786145313; Norm Grads: 75.80706449940214
Training Loss (progress: 0.60): 0.1776385688158632; Norm Grads: 61.81858415458248
Training Loss (progress: 0.70): 0.18722279448882445; Norm Grads: 57.727858787338185
Training Loss (progress: 0.80): 0.19560164894344526; Norm Grads: 69.92843324104513
Training Loss (progress: 0.90): 0.18292642737465098; Norm Grads: 57.760412556079146
Evaluation on validation dataset:
Step 25, mean loss 0.10522125367085501
Step 50, mean loss 0.07015407350473513
Step 75, mean loss 0.08420025592766953
Step 100, mean loss 0.08685302818838725
Step 125, mean loss 0.13470274390654768
Step 150, mean loss 0.13897350008695816
Step 175, mean loss 0.13877766371313952
Step 200, mean loss 0.1957928895284443
Step 225, mean loss 0.26545881171852315
Unrolled forward losses 2.841781029919951
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.0891782125797254
Step 50, mean loss 0.05993546795725726
Step 75, mean loss 0.07161699913177241
Step 100, mean loss 0.08589669782313653
Step 125, mean loss 0.11634583454365183
Step 150, mean loss 0.1401807685126407
Step 175, mean loss 0.17085314451430705
Step 200, mean loss 0.17550339976076607
Step 225, mean loss 0.22953575741399654
Unrolled forward losses 2.914375737669747
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  11:09:33.962403 

Epoch 6
Starting epoch 6...
Training Loss (progress: 0.00): 0.17835540860860125; Norm Grads: 56.647771112629144
Training Loss (progress: 0.10): 0.1730127031402003; Norm Grads: 57.24445049703182
Training Loss (progress: 0.20): 0.17711780795067483; Norm Grads: 60.97578244526451
Training Loss (progress: 0.30): 0.17383401027622625; Norm Grads: 55.62879324441946
Training Loss (progress: 0.40): 0.17604868577360225; Norm Grads: 65.4619805443245
Training Loss (progress: 0.50): 0.17737923269450553; Norm Grads: 64.0554549539459
Training Loss (progress: 0.60): 0.1786693456664478; Norm Grads: 62.77693614906944
Training Loss (progress: 0.70): 0.19303479201274956; Norm Grads: 62.0594255373567
Training Loss (progress: 0.80): 0.17123622289405102; Norm Grads: 51.267231248533285
Training Loss (progress: 0.90): 0.15594302938847007; Norm Grads: 66.19782654469162
Evaluation on validation dataset:
Step 25, mean loss 0.09924765121305397
Step 50, mean loss 0.06583994660100567
Step 75, mean loss 0.08050577311009904
Step 100, mean loss 0.08485947351229019
Step 125, mean loss 0.1312715331272973
Step 150, mean loss 0.1347864492730732
Step 175, mean loss 0.13089155004102515
Step 200, mean loss 0.1844105933174655
Step 225, mean loss 0.2461579682042793
Unrolled forward losses 2.578717014929339
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.08702312761538239
Step 50, mean loss 0.06554810830666057
Step 75, mean loss 0.07111925545568923
Step 100, mean loss 0.08261158494487231
Step 125, mean loss 0.10615155433160993
Step 150, mean loss 0.13171305154162471
Step 175, mean loss 0.1639698116311894
Step 200, mean loss 0.17693854940381654
Step 225, mean loss 0.22849144167029756
Unrolled forward losses 3.069308903199294
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  13:06:00.968895 

Epoch 7
Starting epoch 7...
Training Loss (progress: 0.00): 0.1748507470699332; Norm Grads: 57.84383722162954
Training Loss (progress: 0.10): 0.17037737313655127; Norm Grads: 61.39386066996996
Training Loss (progress: 0.20): 0.17480613700831202; Norm Grads: 70.57005491356749
Training Loss (progress: 0.30): 0.16697228617435209; Norm Grads: 55.837858816098596
Training Loss (progress: 0.40): 0.18438443585450182; Norm Grads: 68.38781389658773
Training Loss (progress: 0.50): 0.15779028915911625; Norm Grads: 52.32606061219609
Training Loss (progress: 0.60): 0.16988770915063958; Norm Grads: 71.33129965571437
Training Loss (progress: 0.70): 0.16451554878179858; Norm Grads: 55.09007323642591
Training Loss (progress: 0.80): 0.16923212167450657; Norm Grads: 66.60320199063054
Training Loss (progress: 0.90): 0.1714306350563511; Norm Grads: 68.55507805167775
Evaluation on validation dataset:
Step 25, mean loss 0.09357044890928724
Step 50, mean loss 0.061447909363720674
Step 75, mean loss 0.07640316215976414
Step 100, mean loss 0.08326309707856955
Step 125, mean loss 0.12966998023000015
Step 150, mean loss 0.12803018891706672
Step 175, mean loss 0.12423461799306648
Step 200, mean loss 0.18597965832647256
Step 225, mean loss 0.23202568174212562
Unrolled forward losses 2.50113429368721
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.0791583950014276
Step 50, mean loss 0.05511295987693182
Step 75, mean loss 0.063323875663277
Step 100, mean loss 0.07732845434833667
Step 125, mean loss 0.09984850932240485
Step 150, mean loss 0.1305194749837173
Step 175, mean loss 0.15691775933962765
Step 200, mean loss 0.1663523795977174
Step 225, mean loss 0.21870876157832184
Unrolled forward losses 2.666212707940456
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  15:02:41.507944 

Epoch 8
Starting epoch 8...
Training Loss (progress: 0.00): 0.16750074562261122; Norm Grads: 67.38969471363785
Training Loss (progress: 0.10): 0.18919936259257294; Norm Grads: 82.41989588313287
Training Loss (progress: 0.20): 0.17019833129633338; Norm Grads: 68.5220419307934
Training Loss (progress: 0.30): 0.17326031939954806; Norm Grads: 75.98409435958293
Training Loss (progress: 0.40): 0.16045427770427884; Norm Grads: 63.628986028078025
Training Loss (progress: 0.50): 0.16446081774006388; Norm Grads: 70.11757886765804
Training Loss (progress: 0.60): 0.1731530114962034; Norm Grads: 69.0271260416375
Training Loss (progress: 0.70): 0.14342752412070528; Norm Grads: 55.5887067296209
Training Loss (progress: 0.80): 0.1601232238934498; Norm Grads: 60.28049857921639
Training Loss (progress: 0.90): 0.16476793420438862; Norm Grads: 55.42107371565605
Evaluation on validation dataset:
Step 25, mean loss 0.0885328204858512
Step 50, mean loss 0.0589320209164706
Step 75, mean loss 0.073054874210532
Step 100, mean loss 0.07851684998950423
Step 125, mean loss 0.12054490595803906
Step 150, mean loss 0.13013118257256912
Step 175, mean loss 0.12486952368681664
Step 200, mean loss 0.17852537721637246
Step 225, mean loss 0.21230540445084248
Unrolled forward losses 2.6034499071527346
Unrolled forward base losses 3.1708552948699085
Epoch 9
Starting epoch 9...
Training Loss (progress: 0.00): 0.15933228267519836; Norm Grads: 54.79648042400327
Training Loss (progress: 0.10): 0.15441514015473526; Norm Grads: 63.60637478626808
Training Loss (progress: 0.20): 0.15725212802837854; Norm Grads: 54.97952869946532
Training Loss (progress: 0.30): 0.16552814884467668; Norm Grads: 64.56515217755269
Training Loss (progress: 0.40): 0.1661919158953799; Norm Grads: 61.39872879627633
Training Loss (progress: 0.50): 0.1688432611748397; Norm Grads: 61.898272246484495
Training Loss (progress: 0.60): 0.1661517244351137; Norm Grads: 63.50355678437901
Training Loss (progress: 0.70): 0.15049122495662806; Norm Grads: 67.06683343568558
Training Loss (progress: 0.80): 0.15311066686243305; Norm Grads: 54.48682192128208
Training Loss (progress: 0.90): 0.14617775387778623; Norm Grads: 73.48258671841235
Evaluation on validation dataset:
Step 25, mean loss 0.08131442288451826
Step 50, mean loss 0.058707965289324565
Step 75, mean loss 0.06874501596949904
Step 100, mean loss 0.07559223218853472
Step 125, mean loss 0.12281371618061718
Step 150, mean loss 0.127492881954877
Step 175, mean loss 0.12022377904343565
Step 200, mean loss 0.18590594546054023
Step 225, mean loss 0.21906403478348438
Unrolled forward losses 2.464897393233972
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.07051459726778406
Step 50, mean loss 0.05310578228253253
Step 75, mean loss 0.06196874023163783
Step 100, mean loss 0.07797789914651707
Step 125, mean loss 0.10040868673910379
Step 150, mean loss 0.1281604816715643
Step 175, mean loss 0.15562118741559955
Step 200, mean loss 0.1628872587800903
Step 225, mean loss 0.21322901877413536
Unrolled forward losses 2.67451550023685
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  18:56:56.200603 

Epoch 10
Starting epoch 10...
Training Loss (progress: 0.00): 0.1499636425048108; Norm Grads: 48.403519351521496
Training Loss (progress: 0.10): 0.14825691335749933; Norm Grads: 50.26125752936429
Training Loss (progress: 0.20): 0.14446045636246918; Norm Grads: 45.99690252106125
Training Loss (progress: 0.30): 0.15802010264390118; Norm Grads: 48.89221234212278
Training Loss (progress: 0.40): 0.1398924762509446; Norm Grads: 49.46256771327179
Training Loss (progress: 0.50): 0.14506231871974187; Norm Grads: 46.16879231478355
Training Loss (progress: 0.60): 0.14418626754878464; Norm Grads: 49.9440109525132
Training Loss (progress: 0.70): 0.14877190327991957; Norm Grads: 53.72584864495014
Training Loss (progress: 0.80): 0.14101798711644262; Norm Grads: 50.619658707600614
Training Loss (progress: 0.90): 0.12605879846784598; Norm Grads: 48.39324566027354
Evaluation on validation dataset:
Step 25, mean loss 0.07884127365411239
Step 50, mean loss 0.05177653428343672
Step 75, mean loss 0.06558505051936783
Step 100, mean loss 0.07036892752218933
Step 125, mean loss 0.11593029805246652
Step 150, mean loss 0.1190993739041906
Step 175, mean loss 0.11360180955801842
Step 200, mean loss 0.16945978601563902
Step 225, mean loss 0.20965682316857762
Unrolled forward losses 2.281117360923005
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.06975598746341795
Step 50, mean loss 0.04886108617655386
Step 75, mean loss 0.05623916079240867
Step 100, mean loss 0.07266955441341799
Step 125, mean loss 0.09394143406491184
Step 150, mean loss 0.12653922777611762
Step 175, mean loss 0.1467914377360382
Step 200, mean loss 0.15709053662280306
Step 225, mean loss 0.2018844085551634
Unrolled forward losses 2.546311223755975
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  20:54:01.994357 

Epoch 11
Starting epoch 11...
Training Loss (progress: 0.00): 0.14284341661662336; Norm Grads: 48.13138403134376
Training Loss (progress: 0.10): 0.14238941440978709; Norm Grads: 46.895175750945036
Training Loss (progress: 0.20): 0.1474998579379174; Norm Grads: 48.29026139424112
Training Loss (progress: 0.30): 0.13884442868585933; Norm Grads: 50.87169047365363
Training Loss (progress: 0.40): 0.12883924275154057; Norm Grads: 52.80272070040271
Training Loss (progress: 0.50): 0.14204302539151287; Norm Grads: 60.50032955385851
Training Loss (progress: 0.60): 0.14148648368309336; Norm Grads: 57.67232637899819
Training Loss (progress: 0.70): 0.14487930736712645; Norm Grads: 46.55789782523695
Training Loss (progress: 0.80): 0.13321892577537747; Norm Grads: 55.75577349402966
Training Loss (progress: 0.90): 0.15424180303560384; Norm Grads: 57.134219062135635
Evaluation on validation dataset:
Step 25, mean loss 0.07776884590871967
Step 50, mean loss 0.05187570247857297
Step 75, mean loss 0.06320571150307268
Step 100, mean loss 0.07124449580925085
Step 125, mean loss 0.11510799305934931
Step 150, mean loss 0.11886266240207921
Step 175, mean loss 0.11209565358397931
Step 200, mean loss 0.17584560462999904
Step 225, mean loss 0.20333556236083755
Unrolled forward losses 2.3163050253439725
Unrolled forward base losses 3.1708552948699085
Epoch 12
Starting epoch 12...
Training Loss (progress: 0.00): 0.14251547679103263; Norm Grads: 47.761747683764455
Training Loss (progress: 0.10): 0.14761679666598723; Norm Grads: 55.9755055751818
Training Loss (progress: 0.20): 0.1449917109267061; Norm Grads: 56.949248568187805
Training Loss (progress: 0.30): 0.14486432169221097; Norm Grads: 48.54056788040277
Training Loss (progress: 0.40): 0.13909587310244836; Norm Grads: 59.649705650274996
Training Loss (progress: 0.50): 0.14135654688390686; Norm Grads: 57.901642922908984
Training Loss (progress: 0.60): 0.14023596123249893; Norm Grads: 55.21981249847488
Training Loss (progress: 0.70): 0.14252660099387499; Norm Grads: 48.08047239598363
Training Loss (progress: 0.80): 0.13265462351205667; Norm Grads: 50.11270876257862
Training Loss (progress: 0.90): 0.1384733438940968; Norm Grads: 52.12313299165208
Evaluation on validation dataset:
Step 25, mean loss 0.07325963543950713
Step 50, mean loss 0.04993821941941908
Step 75, mean loss 0.06137287072666915
Step 100, mean loss 0.06882169434576357
Step 125, mean loss 0.10890477410886573
Step 150, mean loss 0.11568137378778699
Step 175, mean loss 0.10865881536009186
Step 200, mean loss 0.1713069285303649
Step 225, mean loss 0.1997985283799663
Unrolled forward losses 2.174177540440147
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.06383032150683086
Step 50, mean loss 0.04456117921497406
Step 75, mean loss 0.05144164012162256
Step 100, mean loss 0.06812427708936919
Step 125, mean loss 0.08815241354244212
Step 150, mean loss 0.12178097660563643
Step 175, mean loss 0.1390474235609685
Step 200, mean loss 0.14988433664299136
Step 225, mean loss 0.19024152535414995
Unrolled forward losses 2.3931481257678158
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  1 day, 0:48:54.190492 

Epoch 13
Starting epoch 13...
Training Loss (progress: 0.00): 0.1403059310542806; Norm Grads: 55.30474394774849
Training Loss (progress: 0.10): 0.13728284133038257; Norm Grads: 49.597736357373726
Training Loss (progress: 0.20): 0.13285183189978245; Norm Grads: 55.13377209307176
Training Loss (progress: 0.30): 0.14824348464329426; Norm Grads: 51.63146217877137
Training Loss (progress: 0.40): 0.13741914771037234; Norm Grads: 56.411171367603984
Training Loss (progress: 0.50): 0.15592331688712738; Norm Grads: 49.211966516891614
Training Loss (progress: 0.60): 0.13814386625246095; Norm Grads: 49.63027062448484
Training Loss (progress: 0.70): 0.15017779376540105; Norm Grads: 56.535536153941955
Training Loss (progress: 0.80): 0.13762566819363847; Norm Grads: 50.497073896907864
Training Loss (progress: 0.90): 0.12770002652542867; Norm Grads: 46.468553788657694
Evaluation on validation dataset:
Step 25, mean loss 0.07090229328439787
Step 50, mean loss 0.050159657022936746
Step 75, mean loss 0.06208253446621237
Step 100, mean loss 0.06892820332377501
Step 125, mean loss 0.11056335089770225
Step 150, mean loss 0.11798943093528884
Step 175, mean loss 0.10863927816850587
Step 200, mean loss 0.16835173712313387
Step 225, mean loss 0.20416035289340498
Unrolled forward losses 2.2291666574984266
Unrolled forward base losses 3.1708552948699085
Epoch 14
Starting epoch 14...
Training Loss (progress: 0.00): 0.1397390123469669; Norm Grads: 52.378307784117034
Training Loss (progress: 0.10): 0.1395561501811172; Norm Grads: 46.5633210344996
Training Loss (progress: 0.20): 0.1321108031972726; Norm Grads: 46.29616656653573
Training Loss (progress: 0.30): 0.13221200116375606; Norm Grads: 54.77388535827883
Training Loss (progress: 0.40): 0.14513734597009603; Norm Grads: 46.96451872520073
Training Loss (progress: 0.50): 0.12331849887789884; Norm Grads: 50.09160039166894
Training Loss (progress: 0.60): 0.13198151513441247; Norm Grads: 46.9098426047861
Training Loss (progress: 0.70): 0.13998127678605699; Norm Grads: 57.653297365723965
Training Loss (progress: 0.80): 0.14342861967188697; Norm Grads: 58.094199212730146
Training Loss (progress: 0.90): 0.1363828002072318; Norm Grads: 54.989843213341096
Evaluation on validation dataset:
Step 25, mean loss 0.06808038364202207
Step 50, mean loss 0.04799101476537317
Step 75, mean loss 0.06075269862613116
Step 100, mean loss 0.06733425314680545
Step 125, mean loss 0.10881881751690284
Step 150, mean loss 0.11661956730965742
Step 175, mean loss 0.10941598425667834
Step 200, mean loss 0.16115116625598291
Step 225, mean loss 0.20357379235576956
Unrolled forward losses 2.177118099004275
Unrolled forward base losses 3.1708552948699085
Epoch 15
Starting epoch 15...
Training Loss (progress: 0.00): 0.13657395930166844; Norm Grads: 48.62920904124306
Training Loss (progress: 0.10): 0.13139683399647703; Norm Grads: 45.97348630692083
Training Loss (progress: 0.20): 0.12359452360809024; Norm Grads: 41.19981036163172
Training Loss (progress: 0.30): 0.13360348438939587; Norm Grads: 48.12640879999704
Training Loss (progress: 0.40): 0.13916549398992265; Norm Grads: 42.828195139546395
Training Loss (progress: 0.50): 0.12571674769769392; Norm Grads: 45.099908656987026
Training Loss (progress: 0.60): 0.13062495300536; Norm Grads: 42.94074227908263
Training Loss (progress: 0.70): 0.12276502426935168; Norm Grads: 47.519733694060356
Training Loss (progress: 0.80): 0.13924192780425818; Norm Grads: 47.102357896018596
Training Loss (progress: 0.90): 0.1289149112231166; Norm Grads: 46.27866025836738
Evaluation on validation dataset:
Step 25, mean loss 0.06732887831901845
Step 50, mean loss 0.047450848775412904
Step 75, mean loss 0.05863869990619963
Step 100, mean loss 0.06671204447339939
Step 125, mean loss 0.10488934344629039
Step 150, mean loss 0.11522705308514583
Step 175, mean loss 0.10521652971296475
Step 200, mean loss 0.16137284234402927
Step 225, mean loss 0.1935503631364598
Unrolled forward losses 2.1166102365669324
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.058895155087227605
Step 50, mean loss 0.04249199032310752
Step 75, mean loss 0.049996010670865794
Step 100, mean loss 0.06489670409489531
Step 125, mean loss 0.08619091981121291
Step 150, mean loss 0.1169082908616559
Step 175, mean loss 0.13120970902356605
Step 200, mean loss 0.14740090881674187
Step 225, mean loss 0.1844120809719335
Unrolled forward losses 2.2617576577173377
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  1 day, 6:42:14.986885 

Epoch 16
Starting epoch 16...
Training Loss (progress: 0.00): 0.1326722575321128; Norm Grads: 46.91264958673313
Training Loss (progress: 0.10): 0.12699661244055713; Norm Grads: 45.73262754445581
Training Loss (progress: 0.20): 0.1364644506434409; Norm Grads: 46.28221675582049
Training Loss (progress: 0.30): 0.1314054692840843; Norm Grads: 39.983705968291645
Training Loss (progress: 0.40): 0.12366885970722072; Norm Grads: 48.42247142257072
Training Loss (progress: 0.50): 0.13494880083898633; Norm Grads: 42.27926175861727
Training Loss (progress: 0.60): 0.13141214457358172; Norm Grads: 49.274923096321025
Training Loss (progress: 0.70): 0.14231425184868254; Norm Grads: 47.41162840186841
Training Loss (progress: 0.80): 0.13615444955451647; Norm Grads: 47.66520120434536
Training Loss (progress: 0.90): 0.12753882761994462; Norm Grads: 53.22313317987681
Evaluation on validation dataset:
Step 25, mean loss 0.06571429090811465
Step 50, mean loss 0.046328053570322915
Step 75, mean loss 0.05801854045944489
Step 100, mean loss 0.06550819531486729
Step 125, mean loss 0.10397082555368665
Step 150, mean loss 0.11751438945649287
Step 175, mean loss 0.1035872320483863
Step 200, mean loss 0.1629559571568311
Step 225, mean loss 0.19291134255728176
Unrolled forward losses 2.077664354752418
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.05910529807883276
Step 50, mean loss 0.04205112754910307
Step 75, mean loss 0.04945107246916531
Step 100, mean loss 0.06526469370964344
Step 125, mean loss 0.08708012833894913
Step 150, mean loss 0.11709161958429128
Step 175, mean loss 0.13283611110113158
Step 200, mean loss 0.1486528658872789
Step 225, mean loss 0.18601006048863272
Unrolled forward losses 2.293559304802603
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  1 day, 8:42:36.468233 

Epoch 17
Starting epoch 17...
Training Loss (progress: 0.00): 0.10941056320613438; Norm Grads: 46.370316006790155
Training Loss (progress: 0.10): 0.1306895366243138; Norm Grads: 44.88077322232479
Training Loss (progress: 0.20): 0.12998867008998513; Norm Grads: 46.974322455992514
Training Loss (progress: 0.30): 0.136223367092157; Norm Grads: 41.95464834758292
Training Loss (progress: 0.40): 0.1390089954958878; Norm Grads: 41.425990871161964
Training Loss (progress: 0.50): 0.12483902345376112; Norm Grads: 43.50654396360306
Training Loss (progress: 0.60): 0.12066979212594109; Norm Grads: 42.453531497094794
Training Loss (progress: 0.70): 0.12456192992979556; Norm Grads: 40.67271131007456
Training Loss (progress: 0.80): 0.1313809168313973; Norm Grads: 47.485122367062
Training Loss (progress: 0.90): 0.121427262551197; Norm Grads: 41.900478280330034
Evaluation on validation dataset:
Step 25, mean loss 0.06477771470699155
Step 50, mean loss 0.04671440765574816
Step 75, mean loss 0.05760058164516385
Step 100, mean loss 0.06750133264746469
Step 125, mean loss 0.10667272588862715
Step 150, mean loss 0.11241826029409728
Step 175, mean loss 0.10418236658002188
Step 200, mean loss 0.1641439436892177
Step 225, mean loss 0.19373229667568798
Unrolled forward losses 2.117770383918813
Unrolled forward base losses 3.1708552948699085
Epoch 18
Starting epoch 18...
Training Loss (progress: 0.00): 0.11832974397033606; Norm Grads: 46.22244019838298
Training Loss (progress: 0.10): 0.12464818600018293; Norm Grads: 43.908514059372315
Training Loss (progress: 0.20): 0.11654431546895593; Norm Grads: 40.942481101243004
Training Loss (progress: 0.30): 0.1343085428221139; Norm Grads: 42.100943477934244
Training Loss (progress: 0.40): 0.1364088218987214; Norm Grads: 51.06270289078589
Training Loss (progress: 0.50): 0.13468251792375605; Norm Grads: 45.52547310409325
Training Loss (progress: 0.60): 0.13026133007820495; Norm Grads: 43.48841838676589
Training Loss (progress: 0.70): 0.1219243563740118; Norm Grads: 42.80158124071739
Training Loss (progress: 0.80): 0.1287324970061402; Norm Grads: 44.80171042669129
Training Loss (progress: 0.90): 0.1174845797280332; Norm Grads: 41.37333435546354
Evaluation on validation dataset:
Step 25, mean loss 0.06495048159065261
Step 50, mean loss 0.04706556627931949
Step 75, mean loss 0.05934244222393162
Step 100, mean loss 0.06663135934133188
Step 125, mean loss 0.10931726692921447
Step 150, mean loss 0.11267434976580398
Step 175, mean loss 0.10333060620144158
Step 200, mean loss 0.16181712194609368
Step 225, mean loss 0.19189551890068404
Unrolled forward losses 2.12736928309143
Unrolled forward base losses 3.1708552948699085
Epoch 19
Starting epoch 19...
Training Loss (progress: 0.00): 0.13076926279338402; Norm Grads: 51.50157286355994
Training Loss (progress: 0.10): 0.1293096883644246; Norm Grads: 45.2442123734028
Training Loss (progress: 0.20): 0.12296540463946071; Norm Grads: 46.951047518805524
Training Loss (progress: 0.30): 0.13065101604748577; Norm Grads: 44.917614442286215
Training Loss (progress: 0.40): 0.12221795743077843; Norm Grads: 41.40858624154961
Training Loss (progress: 0.50): 0.13045638393799305; Norm Grads: 45.560457131478394
Training Loss (progress: 0.60): 0.12939466133648506; Norm Grads: 42.6048119757855
Training Loss (progress: 0.70): 0.13101557583786502; Norm Grads: 44.59728244365696
Training Loss (progress: 0.80): 0.1182804312213864; Norm Grads: 45.33671674665107
Training Loss (progress: 0.90): 0.12741923676853692; Norm Grads: 44.36290270666605
Evaluation on validation dataset:
Step 25, mean loss 0.0636488901288142
Step 50, mean loss 0.04695477534069549
Step 75, mean loss 0.056811604797746704
Step 100, mean loss 0.06635947377991708
Step 125, mean loss 0.10590005387850657
Step 150, mean loss 0.11542287432294884
Step 175, mean loss 0.10366374792402064
Step 200, mean loss 0.16252189621431726
Step 225, mean loss 0.19131544531185005
Unrolled forward losses 2.165854937676519
Unrolled forward base losses 3.1708552948699085
Epoch 20
Starting epoch 20...
Training Loss (progress: 0.00): 0.12899973529245215; Norm Grads: 47.61752358489219
Training Loss (progress: 0.10): 0.13101620657532267; Norm Grads: 40.75842691790372
Training Loss (progress: 0.20): 0.12199203511210963; Norm Grads: 43.7570309472708
Training Loss (progress: 0.30): 0.12054374213389603; Norm Grads: 43.10242528322225
Training Loss (progress: 0.40): 0.12607334418495686; Norm Grads: 48.54931440410958
Training Loss (progress: 0.50): 0.13324044667899432; Norm Grads: 54.23753081666699
Training Loss (progress: 0.60): 0.13551713072163263; Norm Grads: 49.5397638487051
Training Loss (progress: 0.70): 0.12073848928039445; Norm Grads: 46.110816120861294
Training Loss (progress: 0.80): 0.12993602084166295; Norm Grads: 41.74933903699042
Training Loss (progress: 0.90): 0.1212177250023946; Norm Grads: 45.96113497422323
Evaluation on validation dataset:
Step 25, mean loss 0.0636616474498653
Step 50, mean loss 0.04578293199783896
Step 75, mean loss 0.05603520456872775
Step 100, mean loss 0.06475415288077496
Step 125, mean loss 0.1062592741856811
Step 150, mean loss 0.11446584277101439
Step 175, mean loss 0.1041234428152651
Step 200, mean loss 0.15834107246753154
Step 225, mean loss 0.1909860246331835
Unrolled forward losses 2.1356320671758224
Unrolled forward base losses 3.1708552948699085
Epoch 21
Starting epoch 21...
Training Loss (progress: 0.00): 0.12453788715043894; Norm Grads: 43.982838204279965
Training Loss (progress: 0.10): 0.12182672717760938; Norm Grads: 47.36820853465301
Training Loss (progress: 0.20): 0.12239455898887351; Norm Grads: 45.27155937214538
Training Loss (progress: 0.30): 0.1298875914960299; Norm Grads: 46.68711865835243
Training Loss (progress: 0.40): 0.1290625354013368; Norm Grads: 44.7110052029071
Training Loss (progress: 0.50): 0.1302621894058657; Norm Grads: 46.558688339240554
Training Loss (progress: 0.60): 0.1260109553704225; Norm Grads: 43.36460225779247
Training Loss (progress: 0.70): 0.12973101141806045; Norm Grads: 46.15392277151455
Training Loss (progress: 0.80): 0.13120157596926196; Norm Grads: 49.70049040278288
Training Loss (progress: 0.90): 0.12838280693007667; Norm Grads: 50.3779392518702
Evaluation on validation dataset:
Step 25, mean loss 0.062202326138011896
Step 50, mean loss 0.0453270226624026
Step 75, mean loss 0.05610487885911496
Step 100, mean loss 0.06545873237211854
Step 125, mean loss 0.10520594527453278
Step 150, mean loss 0.11692924149782691
Step 175, mean loss 0.10394392790816295
Step 200, mean loss 0.1589103024590815
Step 225, mean loss 0.19218763028192842
Unrolled forward losses 2.1191120832179693
Unrolled forward base losses 3.1708552948699085
Epoch 22
Starting epoch 22...
Training Loss (progress: 0.00): 0.12099393081951704; Norm Grads: 45.352467567176035
Training Loss (progress: 0.10): 0.12235459935007822; Norm Grads: 44.002413441919174
Training Loss (progress: 0.20): 0.12341706768320529; Norm Grads: 44.32237056403864
Training Loss (progress: 0.30): 0.1347320973089694; Norm Grads: 45.11823287071637
Training Loss (progress: 0.40): 0.12943516965318458; Norm Grads: 40.35055733250631
Training Loss (progress: 0.50): 0.12063934701387946; Norm Grads: 47.10809785014494
Training Loss (progress: 0.60): 0.1342209288752153; Norm Grads: 44.19506632357152
Training Loss (progress: 0.70): 0.13215552487933113; Norm Grads: 54.57957521992212
Training Loss (progress: 0.80): 0.1344756635123348; Norm Grads: 41.313269617952834
Training Loss (progress: 0.90): 0.12753064234831035; Norm Grads: 44.371126495258956
Evaluation on validation dataset:
Step 25, mean loss 0.0608800740208511
Step 50, mean loss 0.04440234146302874
Step 75, mean loss 0.057060965339865585
Step 100, mean loss 0.06613148772518335
Step 125, mean loss 0.10364653814421976
Step 150, mean loss 0.11370885964291227
Step 175, mean loss 0.10334269076411555
Step 200, mean loss 0.16355280070289985
Step 225, mean loss 0.18770290384222518
Unrolled forward losses 2.0662281568183953
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.05436710279878731
Step 50, mean loss 0.04003532930940287
Step 75, mean loss 0.047835997452378855
Step 100, mean loss 0.0643306969634116
Step 125, mean loss 0.08462289697884864
Step 150, mean loss 0.1185376621610917
Step 175, mean loss 0.1278797884976252
Step 200, mean loss 0.1459485507392297
Step 225, mean loss 0.1825207426393316
Unrolled forward losses 2.1569118981421473
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n100_tw25_unrolling2_time13755.pt
Training time:  1 day, 20:50:32.692625 

Epoch 23
Starting epoch 23...
Training Loss (progress: 0.00): 0.12027324770503293; Norm Grads: 43.35973093015779
Training Loss (progress: 0.10): 0.12301943566936979; Norm Grads: 39.782481477419424
Training Loss (progress: 0.20): 0.12402810556296291; Norm Grads: 50.67723295286452
Training Loss (progress: 0.30): 0.1257897199436121; Norm Grads: 47.43545717841034
Training Loss (progress: 0.40): 0.1092826564124652; Norm Grads: 45.0531798648447
Training Loss (progress: 0.50): 0.11939986648226392; Norm Grads: 41.46535546863887
Training Loss (progress: 0.60): 0.13452216109826678; Norm Grads: 40.61900478230714
Training Loss (progress: 0.70): 0.12447181350164943; Norm Grads: 46.68135262184865
Training Loss (progress: 0.80): 0.12479034480860762; Norm Grads: 50.09019331637395
Training Loss (progress: 0.90): 0.13201530972647424; Norm Grads: 45.32709767861813
Evaluation on validation dataset:
Step 25, mean loss 0.06004244392022129
Step 50, mean loss 0.04422164520073141
Step 75, mean loss 0.055268703614362744
Step 100, mean loss 0.06333373999908903
Step 125, mean loss 0.10149844721793624
Step 150, mean loss 0.11146077606894317
Step 175, mean loss 0.10173621705037261
Step 200, mean loss 0.15747465581235898
Step 225, mean loss 0.18486347409408743
Unrolled forward losses 2.083469596321242
Unrolled forward base losses 3.1708552948699085
Epoch 24
Starting epoch 24...
Training Loss (progress: 0.00): 0.12414243674695212; Norm Grads: 42.93528794773956
Training Loss (progress: 0.10): 0.12708887989503742; Norm Grads: 51.760053141376694
Training Loss (progress: 0.20): 0.12054350440444823; Norm Grads: 44.17499475984751
Training Loss (progress: 0.30): 0.12276214740926841; Norm Grads: 42.52884986289336
Training Loss (progress: 0.40): 0.13162655202144935; Norm Grads: 46.01915809294897
