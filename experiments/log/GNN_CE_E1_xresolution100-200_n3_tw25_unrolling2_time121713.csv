Training on dataset data/CE_train_E1.h5
cuda:0
models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Number of parameters: 1031645
Training started at: 2025-01-02 17:13:10
Epoch 0
Starting epoch 0...
Training Loss (progress: 0.00): 1.2445075623343687; Norm Grads: 41.379617644488576
Training Loss (progress: 0.10): 0.2462605115707401; Norm Grads: 200.8049940178375
Training Loss (progress: 0.20): 0.18489227421994656; Norm Grads: 184.53469732947724
Training Loss (progress: 0.30): 0.17185537384012195; Norm Grads: 188.09046268503644
Training Loss (progress: 0.40): 0.15066145309203527; Norm Grads: 181.55791390125546
Training Loss (progress: 0.50): 0.13812203848329266; Norm Grads: 164.9259446868103
Training Loss (progress: 0.60): 0.12565468836562252; Norm Grads: 151.43922259542515
Training Loss (progress: 0.70): 0.12066140156153096; Norm Grads: 156.52067109016335
Training Loss (progress: 0.80): 0.11328658612402205; Norm Grads: 126.4106083094989
Training Loss (progress: 0.90): 0.11154339954728679; Norm Grads: 131.045532510919
Evaluation on validation dataset:
Step 25, mean loss 0.11842105319637733
Step 50, mean loss 0.13366098239415494
Step 75, mean loss 0.1573687918871085
Step 100, mean loss 0.16735766056245532
Step 125, mean loss 0.20707355403169214
Step 150, mean loss 0.21815354657097807
Step 175, mean loss 0.5387827167980177
Step 200, mean loss 0.3320359490315089
Step 225, mean loss 0.42333723801557144
Unrolled forward losses 19.37463875841645
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.10992626237552978
Step 50, mean loss 0.1380738391154465
Step 75, mean loss 0.13069617968608951
Step 100, mean loss 0.14519871391571965
Step 125, mean loss 0.1890585750104845
Step 150, mean loss 0.26178895312477746
Step 175, mean loss 0.9678161151541743
Step 200, mean loss 0.28644853425985767
Step 225, mean loss 0.24330545767455636
Unrolled forward losses 26.73943752268331
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  1:24:52.004149 

Epoch 1
Starting epoch 1...
Training Loss (progress: 0.00): 0.23921554543751275; Norm Grads: 126.77881586662578
Training Loss (progress: 0.10): 0.24060933856988145; Norm Grads: 117.77214615997137
Training Loss (progress: 0.20): 0.24550449368679506; Norm Grads: 99.36703269208618
Training Loss (progress: 0.30): 0.24175305738449118; Norm Grads: 122.2228471394318
Training Loss (progress: 0.40): 0.19824757423404019; Norm Grads: 113.3085114182445
Training Loss (progress: 0.50): 0.19665986149899833; Norm Grads: 115.12233534680618
Training Loss (progress: 0.60): 0.17473804234946705; Norm Grads: 97.8820223260342
Training Loss (progress: 0.70): 0.19335992585368594; Norm Grads: 106.6396110222557
Training Loss (progress: 0.80): 0.14983032932372378; Norm Grads: 88.06467208327881
Training Loss (progress: 0.90): 0.16243938449687745; Norm Grads: 112.11268201672789
Evaluation on validation dataset:
Step 25, mean loss 0.09155446269121305
Step 50, mean loss 0.06628479227234231
Step 75, mean loss 0.0775091945780644
Step 100, mean loss 0.07988458262948916
Step 125, mean loss 0.08409609224785192
Step 150, mean loss 0.09315254258807355
Step 175, mean loss 0.20470950481692268
Step 200, mean loss 0.16772476500821615
Step 225, mean loss 0.20532717955593882
Unrolled forward losses 4.463069118343068
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.08190788626883395
Step 50, mean loss 0.06238369395861802
Step 75, mean loss 0.07271723367126441
Step 100, mean loss 0.07428886302614582
Step 125, mean loss 0.0864637381706582
Step 150, mean loss 0.10085159015306458
Step 175, mean loss 0.3134000193564281
Step 200, mean loss 0.1539728966247787
Step 225, mean loss 0.14032959338652426
Unrolled forward losses 5.082527503977435
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  2:52:55.578023 

Epoch 2
Starting epoch 2...
Training Loss (progress: 0.00): 0.23743457845830454; Norm Grads: 83.70910552298292
Training Loss (progress: 0.10): 0.23171301637554298; Norm Grads: 81.14706788601201
Training Loss (progress: 0.20): 0.2203723397490493; Norm Grads: 86.93957735285163
Training Loss (progress: 0.30): 0.19305309872257478; Norm Grads: 75.88275397476202
Training Loss (progress: 0.40): 0.23872230597574376; Norm Grads: 87.80605429321471
Training Loss (progress: 0.50): 0.22457884382463084; Norm Grads: 79.47383126324162
Training Loss (progress: 0.60): 0.205214870863544; Norm Grads: 89.72628098713902
Training Loss (progress: 0.70): 0.2223480258699378; Norm Grads: 85.85215496130255
Training Loss (progress: 0.80): 0.21718114586642578; Norm Grads: 83.12415589637352
Training Loss (progress: 0.90): 0.21450866401650787; Norm Grads: 68.18925095713621
Evaluation on validation dataset:
Step 25, mean loss 0.09663215916205697
Step 50, mean loss 0.0520758168909933
Step 75, mean loss 0.04716562625425218
Step 100, mean loss 0.04719087254464254
Step 125, mean loss 0.059140632262599385
Step 150, mean loss 0.07057831277560092
Step 175, mean loss 0.1189114934013509
Step 200, mean loss 0.13192860207382995
Step 225, mean loss 0.1320530892830965
Unrolled forward losses 2.9527305852758543
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.0828123193933141
Step 50, mean loss 0.04608857450848279
Step 75, mean loss 0.04379886716966484
Step 100, mean loss 0.04542047162216169
Step 125, mean loss 0.05548230881587482
Step 150, mean loss 0.07049860760750379
Step 175, mean loss 0.14686847218035132
Step 200, mean loss 0.13270994175849263
Step 225, mean loss 0.1231441143134645
Unrolled forward losses 2.9500752825920014
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  4:24:45.206713 

Epoch 3
Starting epoch 3...
Training Loss (progress: 0.00): 0.21323256281746752; Norm Grads: 79.07174927235907
Training Loss (progress: 0.10): 0.19772036957038716; Norm Grads: 90.79296923962238
Training Loss (progress: 0.20): 0.19762499714846765; Norm Grads: 72.49466536989759
Training Loss (progress: 0.30): 0.20295599506256323; Norm Grads: 82.87243702021844
Training Loss (progress: 0.40): 0.20126060961057926; Norm Grads: 95.56915685659708
Training Loss (progress: 0.50): 0.20441509696671706; Norm Grads: 86.33650426435216
Training Loss (progress: 0.60): 0.20189596995104533; Norm Grads: 81.52953595879073
Training Loss (progress: 0.70): 0.17509497580596192; Norm Grads: 78.85921870381895
Training Loss (progress: 0.80): 0.19370967936102307; Norm Grads: 105.27093960783357
Training Loss (progress: 0.90): 0.17441977631282232; Norm Grads: 86.21271054782221
Evaluation on validation dataset:
Step 25, mean loss 0.06984928857218745
Step 50, mean loss 0.04821903769188016
Step 75, mean loss 0.04834062440373155
Step 100, mean loss 0.05245946919359018
Step 125, mean loss 0.061837100501556254
Step 150, mean loss 0.07294363122694246
Step 175, mean loss 0.13653428229966386
Step 200, mean loss 0.13006547256754447
Step 225, mean loss 0.13976572894692327
Unrolled forward losses 2.5017241031448547
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.06113875847040126
Step 50, mean loss 0.04214467054480572
Step 75, mean loss 0.05026252789203633
Step 100, mean loss 0.05073475561999328
Step 125, mean loss 0.06250961026253957
Step 150, mean loss 0.06835760358897662
Step 175, mean loss 0.16026810440625852
Step 200, mean loss 0.12887039337753659
Step 225, mean loss 0.1112549352458961
Unrolled forward losses 2.4105266475598084
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  5:57:27.059780 

Epoch 4
Starting epoch 4...
Training Loss (progress: 0.00): 0.1842754178713815; Norm Grads: 89.85255262644341
Training Loss (progress: 0.10): 0.18007381022197233; Norm Grads: 99.47920702578664
Training Loss (progress: 0.20): 0.18697828332479474; Norm Grads: 79.03826136416012
Training Loss (progress: 0.30): 0.18557826878283476; Norm Grads: 84.80165679688058
Training Loss (progress: 0.40): 0.1824769696107939; Norm Grads: 82.33670016620721
Training Loss (progress: 0.50): 0.14964870823689086; Norm Grads: 79.23407626167612
Training Loss (progress: 0.60): 0.178978198596134; Norm Grads: 82.76397504516083
Training Loss (progress: 0.70): 0.16642771488182337; Norm Grads: 66.24421616381959
Training Loss (progress: 0.80): 0.17569100345879884; Norm Grads: 107.52704558854938
Training Loss (progress: 0.90): 0.18697950827195567; Norm Grads: 91.97701715826737
Evaluation on validation dataset:
Step 25, mean loss 0.06784801307663113
Step 50, mean loss 0.03782439963047551
Step 75, mean loss 0.03970084147898385
Step 100, mean loss 0.04185464681722206
Step 125, mean loss 0.04684283451412502
Step 150, mean loss 0.055611104055980756
Step 175, mean loss 0.1032997384935393
Step 200, mean loss 0.09890680291584954
Step 225, mean loss 0.11628494329838097
Unrolled forward losses 2.1003013988892687
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.05574165751054083
Step 50, mean loss 0.03206060106981766
Step 75, mean loss 0.03388499418086183
Step 100, mean loss 0.039556678165576675
Step 125, mean loss 0.04919031045766119
Step 150, mean loss 0.059361820245998426
Step 175, mean loss 0.1354613747914743
Step 200, mean loss 0.10656478957361754
Step 225, mean loss 0.08749131009061176
Unrolled forward losses 2.1688497925805414
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  7:30:58.141319 

Epoch 5
Starting epoch 5...
Training Loss (progress: 0.00): 0.15474685806957145; Norm Grads: 59.8093459968679
Training Loss (progress: 0.10): 0.1370429081236701; Norm Grads: 67.8576332614799
Training Loss (progress: 0.20): 0.14833398910835843; Norm Grads: 67.53892263699011
Training Loss (progress: 0.30): 0.1357472318812011; Norm Grads: 67.87876687818462
Training Loss (progress: 0.40): 0.1455356774439966; Norm Grads: 72.71903594161415
Training Loss (progress: 0.50): 0.14072735019824445; Norm Grads: 66.3346799082926
Training Loss (progress: 0.60): 0.13733930421109036; Norm Grads: 67.63320711019877
Training Loss (progress: 0.70): 0.13940971577767314; Norm Grads: 62.72997527120836
Training Loss (progress: 0.80): 0.15751188464126165; Norm Grads: 79.58221381087846
Training Loss (progress: 0.90): 0.14413342207730068; Norm Grads: 71.67771122114813
Evaluation on validation dataset:
Step 25, mean loss 0.04339837471951967
Step 50, mean loss 0.023703774159893408
Step 75, mean loss 0.027940568830457577
Step 100, mean loss 0.030297616856661044
Step 125, mean loss 0.03609169569289623
Step 150, mean loss 0.04546169027893814
Step 175, mean loss 0.08783634028543366
Step 200, mean loss 0.0803137626069178
Step 225, mean loss 0.09700105314732474
Unrolled forward losses 1.5751677216803528
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.03804625596795732
Step 50, mean loss 0.021717961320661106
Step 75, mean loss 0.024542646019497412
Step 100, mean loss 0.02776313386861854
Step 125, mean loss 0.03585460506352371
Step 150, mean loss 0.04232817374960838
Step 175, mean loss 0.11612227818233083
Step 200, mean loss 0.09465907029478154
Step 225, mean loss 0.06828479760444114
Unrolled forward losses 1.745491644677672
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  9:05:09.560341 

Epoch 6
Starting epoch 6...
Training Loss (progress: 0.00): 0.14022076973626116; Norm Grads: 75.2944178188751
Training Loss (progress: 0.10): 0.14264352682392534; Norm Grads: 79.57066708360763
Training Loss (progress: 0.20): 0.13510313866276516; Norm Grads: 69.27301479560506
Training Loss (progress: 0.30): 0.14721925863086877; Norm Grads: 70.46304242490571
Training Loss (progress: 0.40): 0.1465241520474334; Norm Grads: 84.34708516568301
Training Loss (progress: 0.50): 0.12310665866814995; Norm Grads: 61.96999015684832
Training Loss (progress: 0.60): 0.13575011340279053; Norm Grads: 63.37824942981592
Training Loss (progress: 0.70): 0.146544918624562; Norm Grads: 57.988771631515036
Training Loss (progress: 0.80): 0.1379149641714509; Norm Grads: 72.02609033716423
Training Loss (progress: 0.90): 0.1287438951148896; Norm Grads: 72.7071581608264
Evaluation on validation dataset:
Step 25, mean loss 0.04083779421480906
Step 50, mean loss 0.02107477863303516
Step 75, mean loss 0.02606422119626091
Step 100, mean loss 0.028587993013455507
Step 125, mean loss 0.032954265872253745
Step 150, mean loss 0.04217282164311388
Step 175, mean loss 0.07721807724877132
Step 200, mean loss 0.07711982581938448
Step 225, mean loss 0.08983071798742653
Unrolled forward losses 1.5725082165134519
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.035948168589551216
Step 50, mean loss 0.02000907875093136
Step 75, mean loss 0.023603973231363685
Step 100, mean loss 0.026771015629078104
Step 125, mean loss 0.03320766393794662
Step 150, mean loss 0.042498492405890896
Step 175, mean loss 0.09016036599518676
Step 200, mean loss 0.0957113644777769
Step 225, mean loss 0.06705071547938722
Unrolled forward losses 1.5715478921003418
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  10:39:51.186515 

Epoch 7
Starting epoch 7...
Training Loss (progress: 0.00): 0.13425334870345376; Norm Grads: 73.85919507704756
Training Loss (progress: 0.10): 0.1312814252057013; Norm Grads: 72.28131021748005
Training Loss (progress: 0.20): 0.12449614400479897; Norm Grads: 88.64908265514204
Training Loss (progress: 0.30): 0.14630499750845385; Norm Grads: 72.37143183113152
Training Loss (progress: 0.40): 0.13487565127332998; Norm Grads: 63.87574169975187
Training Loss (progress: 0.50): 0.12461853149681217; Norm Grads: 72.20643680808675
Training Loss (progress: 0.60): 0.14349710151330597; Norm Grads: 89.43932769765743
Training Loss (progress: 0.70): 0.12721202506689913; Norm Grads: 85.81439544913054
Training Loss (progress: 0.80): 0.13737177161301248; Norm Grads: 77.94938780182285
Training Loss (progress: 0.90): 0.13411376366430766; Norm Grads: 76.57962192995996
Evaluation on validation dataset:
Step 25, mean loss 0.03615742634197632
Step 50, mean loss 0.018762528789675922
Step 75, mean loss 0.025453828234914835
Step 100, mean loss 0.027202417014170893
Step 125, mean loss 0.033129180669060884
Step 150, mean loss 0.04100276806492327
Step 175, mean loss 0.0756125857424986
Step 200, mean loss 0.07059535965201498
Step 225, mean loss 0.090962687546132
Unrolled forward losses 1.436300701703884
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.03164873922650124
Step 50, mean loss 0.01807780287026977
Step 75, mean loss 0.02370277365922592
Step 100, mean loss 0.02618695696431312
Step 125, mean loss 0.03170446430142947
Step 150, mean loss 0.04000142714281467
Step 175, mean loss 0.10358600022970127
Step 200, mean loss 0.08204433934988743
Step 225, mean loss 0.06443755057008121
Unrolled forward losses 1.6855327774914108
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  12:50:16.286270 

Epoch 8
Starting epoch 8...
Training Loss (progress: 0.00): 0.13520045386796567; Norm Grads: 67.10432837798089
Training Loss (progress: 0.10): 0.14885041868046522; Norm Grads: 91.41576553877074
Training Loss (progress: 0.20): 0.13053920028877894; Norm Grads: 81.58057591737996
Training Loss (progress: 0.30): 0.1336434857864344; Norm Grads: 69.40475898195392
Training Loss (progress: 0.40): 0.13197202922948; Norm Grads: 74.49464942850784
Training Loss (progress: 0.50): 0.13126739169787782; Norm Grads: 66.86768076671613
Training Loss (progress: 0.60): 0.13289066242439576; Norm Grads: 73.02585958850477
Training Loss (progress: 0.70): 0.12093136996384515; Norm Grads: 76.54022157901422
Training Loss (progress: 0.80): 0.13732536591108988; Norm Grads: 82.20080250851396
Training Loss (progress: 0.90): 0.134579538851206; Norm Grads: 76.58920956841051
Evaluation on validation dataset:
Step 25, mean loss 0.03261640595139402
Step 50, mean loss 0.020817806011862353
Step 75, mean loss 0.026614790779387276
Step 100, mean loss 0.02702023051842042
Step 125, mean loss 0.03213683813078929
Step 150, mean loss 0.03984505194519711
Step 175, mean loss 0.07369159539567227
Step 200, mean loss 0.07016495873937209
Step 225, mean loss 0.08647063673461616
Unrolled forward losses 1.5084752467818827
Unrolled forward base losses 3.1708552948699085
Epoch 9
Starting epoch 9...
Training Loss (progress: 0.00): 0.12823331711639055; Norm Grads: 66.09116378764811
Training Loss (progress: 0.10): 0.12632478028746064; Norm Grads: 73.01189105573937
Training Loss (progress: 0.20): 0.11959016230904392; Norm Grads: 81.94703901812353
Training Loss (progress: 0.30): 0.1287823845253655; Norm Grads: 76.80707722382829
Training Loss (progress: 0.40): 0.13609255307983248; Norm Grads: 74.71687669155857
Training Loss (progress: 0.50): 0.13392559842761304; Norm Grads: 76.28162052662555
Training Loss (progress: 0.60): 0.13181072217411002; Norm Grads: 79.01592487088169
Training Loss (progress: 0.70): 0.13129463821613763; Norm Grads: 75.7894961970086
Training Loss (progress: 0.80): 0.13413729720318987; Norm Grads: 69.98859168210397
Training Loss (progress: 0.90): 0.12093834186379956; Norm Grads: 74.89436299480009
Evaluation on validation dataset:
Step 25, mean loss 0.02998042079397282
Step 50, mean loss 0.017689955536038634
Step 75, mean loss 0.02375145683928686
Step 100, mean loss 0.026573165162036397
Step 125, mean loss 0.03274181468161329
Step 150, mean loss 0.04118640405538332
Step 175, mean loss 0.0781196059419706
Step 200, mean loss 0.07204635203261227
Step 225, mean loss 0.08461391101623922
Unrolled forward losses 1.4665960084984904
Unrolled forward base losses 3.1708552948699085
Epoch 10
Starting epoch 10...
Training Loss (progress: 0.00): 0.11817743499506844; Norm Grads: 58.37645935276295
Training Loss (progress: 0.10): 0.11571180690948353; Norm Grads: 57.260387121480974
Training Loss (progress: 0.20): 0.12044739854044403; Norm Grads: 57.396573107377264
Training Loss (progress: 0.30): 0.11680334870471448; Norm Grads: 57.86772594745102
Training Loss (progress: 0.40): 0.11461330045657875; Norm Grads: 57.33014423029545
Training Loss (progress: 0.50): 0.12016956064898149; Norm Grads: 72.35206563159761
Training Loss (progress: 0.60): 0.11529104923814743; Norm Grads: 69.47105450147487
Training Loss (progress: 0.70): 0.10836385571224917; Norm Grads: 49.71024152900318
Training Loss (progress: 0.80): 0.12133888526576996; Norm Grads: 62.80659059917403
Training Loss (progress: 0.90): 0.11919506701445184; Norm Grads: 57.95685175568181
Evaluation on validation dataset:
Step 25, mean loss 0.02911858608712268
Step 50, mean loss 0.016336444162139613
Step 75, mean loss 0.02187980003569375
Step 100, mean loss 0.02331034628957138
Step 125, mean loss 0.02760210354931877
Step 150, mean loss 0.03454305334744497
Step 175, mean loss 0.0688260970784194
Step 200, mean loss 0.06502318270279488
Step 225, mean loss 0.07831004137011882
Unrolled forward losses 1.3447095344068107
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.02408160553775076
Step 50, mean loss 0.01513722815736751
Step 75, mean loss 0.019340410123755047
Step 100, mean loss 0.021668905836692707
Step 125, mean loss 0.02744407009474198
Step 150, mean loss 0.034717441761443986
Step 175, mean loss 0.08741695948960822
Step 200, mean loss 0.08018115887691127
Step 225, mean loss 0.05813131164152048
Unrolled forward losses 1.8243656993849338
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  17:45:23.357991 

Epoch 11
Starting epoch 11...
Training Loss (progress: 0.00): 0.10196107744510874; Norm Grads: 62.00969350121131
Training Loss (progress: 0.10): 0.09589360406032744; Norm Grads: 60.92585394083263
Training Loss (progress: 0.20): 0.12060834103085229; Norm Grads: 59.44235985655377
Training Loss (progress: 0.30): 0.11331836129465436; Norm Grads: 54.97601334238444
Training Loss (progress: 0.40): 0.11295920975859636; Norm Grads: 60.92916687733235
Training Loss (progress: 0.50): 0.11250884273847234; Norm Grads: 66.25152662108964
Training Loss (progress: 0.60): 0.11136037193023393; Norm Grads: 53.929908621244614
Training Loss (progress: 0.70): 0.10876273525646245; Norm Grads: 56.767296285930335
Training Loss (progress: 0.80): 0.09542194950666477; Norm Grads: 67.53549173933027
Training Loss (progress: 0.90): 0.11474828225601903; Norm Grads: 56.95078982007536
Evaluation on validation dataset:
Step 25, mean loss 0.026037736386208956
Step 50, mean loss 0.014863059543005002
Step 75, mean loss 0.02017411291421773
Step 100, mean loss 0.022246293242453648
Step 125, mean loss 0.027218558867711018
Step 150, mean loss 0.03417084415613945
Step 175, mean loss 0.061856842671992866
Step 200, mean loss 0.06511303629748556
Step 225, mean loss 0.07435215422768796
Unrolled forward losses 1.346145941991717
Unrolled forward base losses 3.1708552948699085
Epoch 12
Starting epoch 12...
Training Loss (progress: 0.00): 0.10383677449221; Norm Grads: 59.68619109047813
Training Loss (progress: 0.10): 0.10966677647454574; Norm Grads: 62.834965416396564
Training Loss (progress: 0.20): 0.10324921912604555; Norm Grads: 53.63440495046682
Training Loss (progress: 0.30): 0.11109273517572112; Norm Grads: 60.967545256364495
Training Loss (progress: 0.40): 0.1020181557572983; Norm Grads: 52.03448934007262
Training Loss (progress: 0.50): 0.10485632462593368; Norm Grads: 61.75364000825555
Training Loss (progress: 0.60): 0.10618127474355792; Norm Grads: 65.97413902619894
Training Loss (progress: 0.70): 0.10813194867612286; Norm Grads: 67.09718286772384
Training Loss (progress: 0.80): 0.10916216083793809; Norm Grads: 65.34282284983276
Training Loss (progress: 0.90): 0.12398754535548745; Norm Grads: 60.825960605413854
Evaluation on validation dataset:
Step 25, mean loss 0.02591788830119477
Step 50, mean loss 0.015038514568634651
Step 75, mean loss 0.020783452569564147
Step 100, mean loss 0.022615528132764712
Step 125, mean loss 0.027359456648548816
Step 150, mean loss 0.0339715702573785
Step 175, mean loss 0.06517799128334462
Step 200, mean loss 0.06468194708836497
Step 225, mean loss 0.07592874083384063
Unrolled forward losses 1.3288094756199134
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.02280031411950974
Step 50, mean loss 0.015588142388786188
Step 75, mean loss 0.018776815294159255
Step 100, mean loss 0.021133657740974476
Step 125, mean loss 0.027766198068752752
Step 150, mean loss 0.035458802332291456
Step 175, mean loss 0.0804370882875107
Step 200, mean loss 0.07870008496095914
Step 225, mean loss 0.05564536905830621
Unrolled forward losses 1.6101075637492936
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  21:00:30.394290 

Epoch 13
Starting epoch 13...
Training Loss (progress: 0.00): 0.11587406877912344; Norm Grads: 69.87673093635335
Training Loss (progress: 0.10): 0.1062498594404479; Norm Grads: 60.45689368653366
Training Loss (progress: 0.20): 0.11032448705862259; Norm Grads: 68.8656320175127
Training Loss (progress: 0.30): 0.10809148807409287; Norm Grads: 72.79191650417685
Training Loss (progress: 0.40): 0.11578376369137833; Norm Grads: 63.627491093256744
Training Loss (progress: 0.50): 0.10831341107593782; Norm Grads: 68.78539888298721
Training Loss (progress: 0.60): 0.11127034491534958; Norm Grads: 69.95241101342332
Training Loss (progress: 0.70): 0.10649383538711824; Norm Grads: 60.400477255831255
Training Loss (progress: 0.80): 0.11221303097433441; Norm Grads: 65.87920401009309
Training Loss (progress: 0.90): 0.11886825028316252; Norm Grads: 61.762334001702186
Evaluation on validation dataset:
Step 25, mean loss 0.024421055692690045
Step 50, mean loss 0.014822744278392967
Step 75, mean loss 0.020448174918506406
Step 100, mean loss 0.02156799889242105
Step 125, mean loss 0.026999792752603266
Step 150, mean loss 0.03324834725981736
Step 175, mean loss 0.06209973466895618
Step 200, mean loss 0.06202722846896428
Step 225, mean loss 0.07461239797831164
Unrolled forward losses 1.3262843100185586
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.022160542509662984
Step 50, mean loss 0.013736120484185377
Step 75, mean loss 0.017379619813297702
Step 100, mean loss 0.021047754525925298
Step 125, mean loss 0.02702520055473523
Step 150, mean loss 0.03427684899690461
Step 175, mean loss 0.0753387961366743
Step 200, mean loss 0.07759371789061593
Step 225, mean loss 0.05578349491482318
Unrolled forward losses 1.5405557423553824
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  22:40:45.491093 

Epoch 14
Starting epoch 14...
Training Loss (progress: 0.00): 0.10739515713244817; Norm Grads: 58.21191644630753
Training Loss (progress: 0.10): 0.09048219031924144; Norm Grads: 62.53614654279701
Training Loss (progress: 0.20): 0.10833637267772593; Norm Grads: 56.45350110881113
Training Loss (progress: 0.30): 0.1046847511685505; Norm Grads: 54.52948728335471
Training Loss (progress: 0.40): 0.10705831035551579; Norm Grads: 59.317670916666984
Training Loss (progress: 0.50): 0.11064718087833254; Norm Grads: 64.61011582928496
Training Loss (progress: 0.60): 0.11925978100993342; Norm Grads: 74.0823717231096
Training Loss (progress: 0.70): 0.10912799320598833; Norm Grads: 59.40428518360583
Training Loss (progress: 0.80): 0.1070695954579211; Norm Grads: 54.96872826655031
Training Loss (progress: 0.90): 0.10923230619693314; Norm Grads: 61.81378493688692
Evaluation on validation dataset:
Step 25, mean loss 0.023440489180277323
Step 50, mean loss 0.015534574402475677
Step 75, mean loss 0.019372334913192284
Step 100, mean loss 0.021342508429261466
Step 125, mean loss 0.026903616006299987
Step 150, mean loss 0.03398899608189994
Step 175, mean loss 0.06435022107901696
Step 200, mean loss 0.06397095331625005
Step 225, mean loss 0.0735360271896459
Unrolled forward losses 1.3016795268973194
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.02042335710944941
Step 50, mean loss 0.014446984846552949
Step 75, mean loss 0.017507685457429124
Step 100, mean loss 0.02112918308274008
Step 125, mean loss 0.026368548087132872
Step 150, mean loss 0.03502803724874831
Step 175, mean loss 0.07375344951180247
Step 200, mean loss 0.07623957120893388
Step 225, mean loss 0.05687308657233429
Unrolled forward losses 1.7419019589987144
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  1 day, 0:19:10.510686 

Epoch 15
Starting epoch 15...
Training Loss (progress: 0.00): 0.11295790311811886; Norm Grads: 49.10841698588733
Training Loss (progress: 0.10): 0.11217083691526315; Norm Grads: 49.59725825360581
Training Loss (progress: 0.20): 0.1104496118658454; Norm Grads: 54.898652087153145
Training Loss (progress: 0.30): 0.10205173540802459; Norm Grads: 51.84928882403893
Training Loss (progress: 0.40): 0.1039876068382539; Norm Grads: 52.382644251807605
Training Loss (progress: 0.50): 0.10496828173642939; Norm Grads: 53.83491320527621
Training Loss (progress: 0.60): 0.114316280208996; Norm Grads: 60.18887650739498
Training Loss (progress: 0.70): 0.1081627764006202; Norm Grads: 45.65856980272583
Training Loss (progress: 0.80): 0.10150615715625717; Norm Grads: 49.8090294918597
Training Loss (progress: 0.90): 0.10795211838250311; Norm Grads: 50.77319692709856
Evaluation on validation dataset:
Step 25, mean loss 0.022648564538252213
Step 50, mean loss 0.014000754901753277
Step 75, mean loss 0.01883367941869289
Step 100, mean loss 0.02103008427693171
Step 125, mean loss 0.025739944450243912
Step 150, mean loss 0.032792057382539846
Step 175, mean loss 0.06353352153555217
Step 200, mean loss 0.06111546660559677
Step 225, mean loss 0.07083586405631068
Unrolled forward losses 1.2711804993319338
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.020827614433522594
Step 50, mean loss 0.013554593609675838
Step 75, mean loss 0.016998400394669405
Step 100, mean loss 0.01976758044500184
Step 125, mean loss 0.025483396264755747
Step 150, mean loss 0.03214263603439159
Step 175, mean loss 0.07243519721527512
Step 200, mean loss 0.0752358847124179
Step 225, mean loss 0.053941364078060064
Unrolled forward losses 1.7007966792617937
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  1 day, 1:57:46.790539 

Epoch 16
Starting epoch 16...
Training Loss (progress: 0.00): 0.10315392244007288; Norm Grads: 52.25218388179898
Training Loss (progress: 0.10): 0.10560278509236844; Norm Grads: 51.065780130028834
Training Loss (progress: 0.20): 0.1075543774129572; Norm Grads: 52.47482096839338
Training Loss (progress: 0.30): 0.10597074722176317; Norm Grads: 47.98894394703302
Training Loss (progress: 0.40): 0.10552201507381245; Norm Grads: 53.85185187132573
Training Loss (progress: 0.50): 0.10416068320266007; Norm Grads: 62.501555076014235
Training Loss (progress: 0.60): 0.11543185519595256; Norm Grads: 54.6977226471705
Training Loss (progress: 0.70): 0.09518251267006006; Norm Grads: 52.55531076717751
Training Loss (progress: 0.80): 0.0997231660598167; Norm Grads: 51.01418579671735
Training Loss (progress: 0.90): 0.10500051778024096; Norm Grads: 48.98104897259061
Evaluation on validation dataset:
Step 25, mean loss 0.02381269327131956
Step 50, mean loss 0.014243682534110904
Step 75, mean loss 0.018805403954477003
Step 100, mean loss 0.020650194386047215
Step 125, mean loss 0.025583256883856473
Step 150, mean loss 0.0317636583042632
Step 175, mean loss 0.05782353614513559
Step 200, mean loss 0.06114854649383233
Step 225, mean loss 0.07018899505744719
Unrolled forward losses 1.302611486330072
Unrolled forward base losses 3.1708552948699085
Epoch 17
Starting epoch 17...
Training Loss (progress: 0.00): 0.1078542108222583; Norm Grads: 49.47863965485716
Training Loss (progress: 0.10): 0.10742535393832915; Norm Grads: 71.20071262084426
Training Loss (progress: 0.20): 0.11715620761931078; Norm Grads: 53.829702226086624
Training Loss (progress: 0.30): 0.1076888440542823; Norm Grads: 60.7503857804095
Training Loss (progress: 0.40): 0.10054452771702636; Norm Grads: 52.84813127021895
Training Loss (progress: 0.50): 0.10785032373724342; Norm Grads: 51.19540460920537
Training Loss (progress: 0.60): 0.10750922828119554; Norm Grads: 51.39445788929742
Training Loss (progress: 0.70): 0.11655361977723931; Norm Grads: 49.89305776590162
Training Loss (progress: 0.80): 0.11342469431113202; Norm Grads: 54.44417789481937
Training Loss (progress: 0.90): 0.10122746085995095; Norm Grads: 46.87565538126853
Evaluation on validation dataset:
Step 25, mean loss 0.022540033279957794
Step 50, mean loss 0.01446609951614209
Step 75, mean loss 0.018455142696438435
Step 100, mean loss 0.020686799322689572
Step 125, mean loss 0.025639668669230058
Step 150, mean loss 0.0317037601086019
Step 175, mean loss 0.05855891863020288
Step 200, mean loss 0.06042956048818102
Step 225, mean loss 0.06965683805968907
Unrolled forward losses 1.2431820314630444
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.019127085618472924
Step 50, mean loss 0.013343009866971341
Step 75, mean loss 0.01614999600995256
Step 100, mean loss 0.01983205310025164
Step 125, mean loss 0.025342078659140954
Step 150, mean loss 0.03189522134199668
Step 175, mean loss 0.07241649877883868
Step 200, mean loss 0.0690786044675302
Step 225, mean loss 0.05356914242320512
Unrolled forward losses 1.644439508832305
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  1 day, 5:15:28.068849 

Epoch 18
Starting epoch 18...
Training Loss (progress: 0.00): 0.10728620563149696; Norm Grads: 55.430446538218916
Training Loss (progress: 0.10): 0.10796308870980564; Norm Grads: 52.33401169721843
Training Loss (progress: 0.20): 0.10806929461791977; Norm Grads: 54.7206897943442
Training Loss (progress: 0.30): 0.10836454432362047; Norm Grads: 52.285413342819965
Training Loss (progress: 0.40): 0.10246513792222844; Norm Grads: 52.82750744622492
Training Loss (progress: 0.50): 0.09589322726590754; Norm Grads: 44.51417288969797
Training Loss (progress: 0.60): 0.10872130987453847; Norm Grads: 53.776992778301064
Training Loss (progress: 0.70): 0.10491194047573421; Norm Grads: 52.571416177393175
Training Loss (progress: 0.80): 0.10771624831266863; Norm Grads: 53.74963098827761
Training Loss (progress: 0.90): 0.111366739210607; Norm Grads: 58.97741660522327
Evaluation on validation dataset:
Step 25, mean loss 0.02165653270464299
Step 50, mean loss 0.013539683221329184
Step 75, mean loss 0.018478434248594168
Step 100, mean loss 0.020553764114710903
Step 125, mean loss 0.025416032697880293
Step 150, mean loss 0.031957469683925196
Step 175, mean loss 0.06068692988891947
Step 200, mean loss 0.060040338979426525
Step 225, mean loss 0.06915915768221909
Unrolled forward losses 1.2165783069741938
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.018705226482386105
Step 50, mean loss 0.013078640155417116
Step 75, mean loss 0.016312835602169888
Step 100, mean loss 0.01954096902398736
Step 125, mean loss 0.02502078802682202
Step 150, mean loss 0.031120002372004997
Step 175, mean loss 0.06965255699693376
Step 200, mean loss 0.0762183813975465
Step 225, mean loss 0.052829990513175676
Unrolled forward losses 1.729388266703322
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time121713.pt
Training time:  1 day, 6:54:42.543502 

Epoch 19
Starting epoch 19...
Training Loss (progress: 0.00): 0.11800861235616537; Norm Grads: 64.22196749443468
Training Loss (progress: 0.10): 0.10619135298027457; Norm Grads: 54.48163784025363
Training Loss (progress: 0.20): 0.10802179128805972; Norm Grads: 51.16515761874417
Training Loss (progress: 0.30): 0.11063447578230201; Norm Grads: 50.76806451533822
Training Loss (progress: 0.40): 0.11004837976555461; Norm Grads: 49.08436902647585
Training Loss (progress: 0.50): 0.10254889509321832; Norm Grads: 50.83306012023589
Training Loss (progress: 0.60): 0.09828830955463301; Norm Grads: 51.60294976681892
Training Loss (progress: 0.70): 0.11010338775460475; Norm Grads: 50.87078793722365
Training Loss (progress: 0.80): 0.10870927908972358; Norm Grads: 55.65551397110272
Training Loss (progress: 0.90): 0.11062331583009678; Norm Grads: 47.88356961221442
Evaluation on validation dataset:
Step 25, mean loss 0.021754552266070974
Step 50, mean loss 0.013360813415213283
Step 75, mean loss 0.01846488368469923
Step 100, mean loss 0.020373996823028352
Step 125, mean loss 0.02513081988704016
Step 150, mean loss 0.03164954674726404
Step 175, mean loss 0.058789551429739566
Step 200, mean loss 0.05859283523031819
Step 225, mean loss 0.06966937713061781
Unrolled forward losses 1.2664068048476294
Unrolled forward base losses 3.1708552948699085
Epoch 20
Starting epoch 20...
Training Loss (progress: 0.00): 0.10071414242975789; Norm Grads: 50.84922532648851
Training Loss (progress: 0.10): 0.10335360308423452; Norm Grads: 52.40235786360393
Training Loss (progress: 0.20): 0.10001592251711247; Norm Grads: 51.559550740632545
Training Loss (progress: 0.30): 0.09942103742472666; Norm Grads: 54.16805189731374
Training Loss (progress: 0.40): 0.10085115144387247; Norm Grads: 51.79504630950403
Training Loss (progress: 0.50): 0.1000438303543733; Norm Grads: 52.70143229708256
Training Loss (progress: 0.60): 0.1121623942616355; Norm Grads: 65.20028964905885
Training Loss (progress: 0.70): 0.11211383484439125; Norm Grads: 55.15030528654598
Training Loss (progress: 0.80): 0.09593180983562397; Norm Grads: 50.17762396412273
Training Loss (progress: 0.90): 0.09438279183479427; Norm Grads: 48.94660700382216
Evaluation on validation dataset:
Step 25, mean loss 0.020993292911787934
Step 50, mean loss 0.013546832946066567
Step 75, mean loss 0.01823582733385254
Step 100, mean loss 0.020249902867681024
Step 125, mean loss 0.024945303239887223
Step 150, mean loss 0.030958699535739063
Step 175, mean loss 0.05556452713212355
Step 200, mean loss 0.058400866811314664
Step 225, mean loss 0.06847592870288871
Unrolled forward losses 1.2559966891381276
Unrolled forward base losses 3.1708552948699085
Epoch 21
Starting epoch 21...
Training Loss (progress: 0.00): 0.09538733466405458; Norm Grads: 51.423065187999896
Training Loss (progress: 0.10): 0.10143361084006412; Norm Grads: 49.147334333662606
Training Loss (progress: 0.20): 0.10065570483940965; Norm Grads: 52.6974760132751
Training Loss (progress: 0.30): 0.10966038334852929; Norm Grads: 48.03481941733119
Training Loss (progress: 0.40): 0.09290004732186614; Norm Grads: 53.48659244242418
Training Loss (progress: 0.50): 0.09977432083849673; Norm Grads: 56.29367161297237
Training Loss (progress: 0.60): 0.10413381346940574; Norm Grads: 58.04599997525385
Training Loss (progress: 0.70): 0.10083989126068167; Norm Grads: 52.44982380403471
Training Loss (progress: 0.80): 0.1068261881203188; Norm Grads: 51.561608139558096
Training Loss (progress: 0.90): 0.10803123273522502; Norm Grads: 59.739047583235966
Evaluation on validation dataset:
Step 25, mean loss 0.02061003480513866
Step 50, mean loss 0.014228591798105806
Step 75, mean loss 0.017879351577187525
Step 100, mean loss 0.020042435886308087
Step 125, mean loss 0.025145821379261822
Step 150, mean loss 0.03184788324149132
Step 175, mean loss 0.0570149480566092
Step 200, mean loss 0.05962669283332642
Step 225, mean loss 0.06842181030488013
Unrolled forward losses 1.234821875593813
Unrolled forward base losses 3.1708552948699085
Epoch 22
Starting epoch 22...
Training Loss (progress: 0.00): 0.11056928825811918; Norm Grads: 53.069142988698836
Training Loss (progress: 0.10): 0.09260011378967344; Norm Grads: 52.795752793649775
Training Loss (progress: 0.20): 0.10866201699909434; Norm Grads: 53.53603342321924
Training Loss (progress: 0.30): 0.10461516131715126; Norm Grads: 49.329538922950825
Training Loss (progress: 0.40): 0.10420731477878736; Norm Grads: 54.292293925236706
Training Loss (progress: 0.50): 0.10300445175180534; Norm Grads: 55.467462545462276
Training Loss (progress: 0.60): 0.09715131265006062; Norm Grads: 51.18620640753878
Training Loss (progress: 0.70): 0.10143062761194878; Norm Grads: 54.51407926947784
Training Loss (progress: 0.80): 0.09220345203409708; Norm Grads: 49.95971966586035
Training Loss (progress: 0.90): 0.10086049851850146; Norm Grads: 51.79518191184749
Evaluation on validation dataset:
Step 25, mean loss 0.02020655154222686
Step 50, mean loss 0.013661657657540467
Step 75, mean loss 0.01781759415059064
Step 100, mean loss 0.020637495846320627
Step 125, mean loss 0.025404305307896127
Step 150, mean loss 0.031454117401091185
Step 175, mean loss 0.05608476340948627
Step 200, mean loss 0.05871431799457521
Step 225, mean loss 0.06751084638746158
Unrolled forward losses 1.2353438832552304
Unrolled forward base losses 3.1708552948699085
Epoch 23
Starting epoch 23...
Training Loss (progress: 0.00): 0.08879896262683967; Norm Grads: 50.34308417034582
Training Loss (progress: 0.10): 0.107316901083621; Norm Grads: 49.318277045853236
Training Loss (progress: 0.20): 0.09493378409660776; Norm Grads: 56.21016422701439
Training Loss (progress: 0.30): 0.11148383547133715; Norm Grads: 51.90521212334227
Training Loss (progress: 0.40): 0.09905494839941471; Norm Grads: 57.54486621090531
Training Loss (progress: 0.50): 0.10932860954524018; Norm Grads: 51.32169432918609
Training Loss (progress: 0.60): 0.10005945598585868; Norm Grads: 52.270600019754504
Training Loss (progress: 0.70): 0.10760192261367177; Norm Grads: 54.31286704597213
Training Loss (progress: 0.80): 0.09948318997248216; Norm Grads: 57.52427251445403
Training Loss (progress: 0.90): 0.10103294852343336; Norm Grads: 62.64061504719363
Evaluation on validation dataset:
Step 25, mean loss 0.02007813700255143
Step 50, mean loss 0.013558541927198499
Step 75, mean loss 0.017988881272712802
Step 100, mean loss 0.02037669945714514
Step 125, mean loss 0.025220973089983615
Step 150, mean loss 0.0315295761619008
Step 175, mean loss 0.054566590891833466
Step 200, mean loss 0.05936263622834551
Step 225, mean loss 0.06665196126149191
Unrolled forward losses 1.2809016155627624
Unrolled forward base losses 3.1708552948699085
Epoch 24
Starting epoch 24...
Training Loss (progress: 0.00): 0.10588721476718539; Norm Grads: 57.101698228950404
Training Loss (progress: 0.10): 0.10121203633275033; Norm Grads: 56.197626511243655
Training Loss (progress: 0.20): 0.10137278763834237; Norm Grads: 51.63905389366514
Training Loss (progress: 0.30): 0.1098525289337268; Norm Grads: 58.75212324216078
Training Loss (progress: 0.40): 0.10651119051230651; Norm Grads: 57.01133587671681
Training Loss (progress: 0.50): 0.10024687630190209; Norm Grads: 58.638748825017444
Training Loss (progress: 0.60): 0.09871083111437723; Norm Grads: 59.017338256719
Training Loss (progress: 0.70): 0.09920074241887408; Norm Grads: 48.27749957159518
Training Loss (progress: 0.80): 0.10756638909027258; Norm Grads: 62.47391853522932
Training Loss (progress: 0.90): 0.09398820543962012; Norm Grads: 49.09857342700439
Evaluation on validation dataset:
Step 25, mean loss 0.020313601694703982
Step 50, mean loss 0.013605447930103293
Step 75, mean loss 0.01788241227815367
Step 100, mean loss 0.019511994886652402
Step 125, mean loss 0.02473146610122047
Step 150, mean loss 0.031175909677607954
Step 175, mean loss 0.055858312781784415
Step 200, mean loss 0.05867120571913843
Step 225, mean loss 0.06729292849805937
Unrolled forward losses 1.2541435423168745
Unrolled forward base losses 3.1708552948699085
Test loss: 1.729388266703322
Training time (until epoch 18):  {datetime.timedelta(days=1, seconds=24882, microseconds=543502)}
