Training on dataset data/CE_train_E1.h5
cuda:0
models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Number of parameters: 1031645
Training started at: 2025-01-02 17:13:19
Epoch 0
Starting epoch 0...
Training Loss (progress: 0.00): 1.3216421135051342; Norm Grads: 31.388066985443167
Training Loss (progress: 0.10): 0.2694856915805152; Norm Grads: 200.25799748429552
Training Loss (progress: 0.20): 0.21515737523310446; Norm Grads: 160.07839300649852
Training Loss (progress: 0.30): 0.17924179943679597; Norm Grads: 166.51682490497072
Training Loss (progress: 0.40): 0.16394536010314295; Norm Grads: 148.80758936736453
Training Loss (progress: 0.50): 0.1354727997459161; Norm Grads: 127.53830672998653
Training Loss (progress: 0.60): 0.13547290119726796; Norm Grads: 175.0280965122501
Training Loss (progress: 0.70): 0.13060528702258956; Norm Grads: 143.52904934892902
Training Loss (progress: 0.80): 0.12885000969474852; Norm Grads: 134.54167298819812
Training Loss (progress: 0.90): 0.11647162759475596; Norm Grads: 148.3260264127577
Evaluation on validation dataset:
Step 25, mean loss 0.12540080101373266
Step 50, mean loss 0.13494760697746622
Step 75, mean loss 0.13890350939831975
Step 100, mean loss 0.14471811997764578
Step 125, mean loss 0.1731913048487796
Step 150, mean loss 0.19710464437218436
Step 175, mean loss 0.43080793630341646
Step 200, mean loss 0.30277869448560224
Step 225, mean loss 0.3891199114658095
Unrolled forward losses 13.917058031302677
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.12352149151473454
Step 50, mean loss 0.12729506858609474
Step 75, mean loss 0.12605249357693546
Step 100, mean loss 0.14775488336994563
Step 125, mean loss 0.18611529189773057
Step 150, mean loss 0.1769110667800682
Step 175, mean loss 0.6485132449692098
Step 200, mean loss 0.296364963975461
Step 225, mean loss 0.2588115241773668
Unrolled forward losses 15.79362915494454
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  1:24:22.487397 

Epoch 1
Starting epoch 1...
Training Loss (progress: 0.00): 0.20346625684791753; Norm Grads: 133.98998830523297
Training Loss (progress: 0.10): 0.22410523918097136; Norm Grads: 131.1331997337132
Training Loss (progress: 0.20): 0.19821376465246918; Norm Grads: 129.17212306937938
Training Loss (progress: 0.30): 0.21160108992746937; Norm Grads: 120.16397923478674
Training Loss (progress: 0.40): 0.21470106406852293; Norm Grads: 97.0536029841266
Training Loss (progress: 0.50): 0.18936127192095756; Norm Grads: 109.52499639520789
Training Loss (progress: 0.60): 0.18919285125320956; Norm Grads: 106.17635135934775
Training Loss (progress: 0.70): 0.16656984725249857; Norm Grads: 87.2780958760224
Training Loss (progress: 0.80): 0.18397466023673922; Norm Grads: 113.30416795415051
Training Loss (progress: 0.90): 0.1568197582183276; Norm Grads: 105.15710961919041
Evaluation on validation dataset:
Step 25, mean loss 0.09763931619388533
Step 50, mean loss 0.06919513117926704
Step 75, mean loss 0.07956586316205717
Step 100, mean loss 0.09522057340024741
Step 125, mean loss 0.1033397951346027
Step 150, mean loss 0.13290684665408292
Step 175, mean loss 0.19183179156040597
Step 200, mean loss 0.2079719677268206
Step 225, mean loss 0.20553049874989257
Unrolled forward losses 3.3748556458363534
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.0884304307909311
Step 50, mean loss 0.07088467089230095
Step 75, mean loss 0.07014862139787686
Step 100, mean loss 0.078736194409465
Step 125, mean loss 0.10162557639438738
Step 150, mean loss 0.10705659758727676
Step 175, mean loss 0.2692607739423808
Step 200, mean loss 0.16607929370461524
Step 225, mean loss 0.1934119397866988
Unrolled forward losses 3.1093915987436382
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  2:53:12.257349 

Epoch 2
Starting epoch 2...
Training Loss (progress: 0.00): 0.2288963344902544; Norm Grads: 80.23415403735038
Training Loss (progress: 0.10): 0.20704342079731208; Norm Grads: 79.32525986798105
Training Loss (progress: 0.20): 0.22172035256547545; Norm Grads: 89.83542640975537
Training Loss (progress: 0.30): 0.22744440487724338; Norm Grads: 91.45194606638987
Training Loss (progress: 0.40): 0.2032278671810181; Norm Grads: 92.48214365869325
Training Loss (progress: 0.50): 0.19325914627340157; Norm Grads: 83.16019929563397
Training Loss (progress: 0.60): 0.18434847908047958; Norm Grads: 93.01736090014724
Training Loss (progress: 0.70): 0.20538979974605326; Norm Grads: 90.57858870931256
Training Loss (progress: 0.80): 0.22798280599091528; Norm Grads: 83.35842959110428
Training Loss (progress: 0.90): 0.17791123172652587; Norm Grads: 85.14371887256173
Evaluation on validation dataset:
Step 25, mean loss 0.0850952424705103
Step 50, mean loss 0.04291635044598083
Step 75, mean loss 0.050349051834355577
Step 100, mean loss 0.05431808021573052
Step 125, mean loss 0.059755087273080064
Step 150, mean loss 0.0749088784147888
Step 175, mean loss 0.15098493271401675
Step 200, mean loss 0.12815262956705853
Step 225, mean loss 0.1446626883695994
Unrolled forward losses 2.2454009458490587
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.07590949744121339
Step 50, mean loss 0.03884074416603108
Step 75, mean loss 0.040050742677883104
Step 100, mean loss 0.0483514771898185
Step 125, mean loss 0.06136321479533963
Step 150, mean loss 0.06694453633760566
Step 175, mean loss 0.10613364989465866
Step 200, mean loss 0.11485333657014667
Step 225, mean loss 0.13553242757805042
Unrolled forward losses 2.5609738273277753
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  4:26:18.924611 

Epoch 3
Starting epoch 3...
Training Loss (progress: 0.00): 0.19637989927688654; Norm Grads: 76.02675066622724
Training Loss (progress: 0.10): 0.20047469335808268; Norm Grads: 64.14016291837798
Training Loss (progress: 0.20): 0.18607673893242482; Norm Grads: 90.29523265999568
Training Loss (progress: 0.30): 0.20533708522749056; Norm Grads: 81.90256821067452
Training Loss (progress: 0.40): 0.17902783869071517; Norm Grads: 98.98799565146487
Training Loss (progress: 0.50): 0.17593792731463248; Norm Grads: 82.67795125816582
Training Loss (progress: 0.60): 0.1783211649167894; Norm Grads: 89.46231803008378
Training Loss (progress: 0.70): 0.19378232826817032; Norm Grads: 91.83841051179893
Training Loss (progress: 0.80): 0.17893542249928193; Norm Grads: 85.80866016840564
Training Loss (progress: 0.90): 0.1972831614859545; Norm Grads: 89.67922950629622
Evaluation on validation dataset:
Step 25, mean loss 0.08020571753668836
Step 50, mean loss 0.03815305131413313
Step 75, mean loss 0.05020392475100847
Step 100, mean loss 0.06145347907778834
Step 125, mean loss 0.06955059237109812
Step 150, mean loss 0.08213879909373163
Step 175, mean loss 0.1386758386383565
Step 200, mean loss 0.1236138274685745
Step 225, mean loss 0.1496030496978038
Unrolled forward losses 2.123670213184247
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.06887366954052077
Step 50, mean loss 0.03474078326973469
Step 75, mean loss 0.0419097618291521
Step 100, mean loss 0.05515475728570243
Step 125, mean loss 0.06732364091807849
Step 150, mean loss 0.08431486181190831
Step 175, mean loss 0.1088593538022191
Step 200, mean loss 0.12997106402288047
Step 225, mean loss 0.15458822900682023
Unrolled forward losses 2.3471707353041116
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  6:00:03.089042 

Epoch 4
Starting epoch 4...
Training Loss (progress: 0.00): 0.18315689583255698; Norm Grads: 84.95561818825162
Training Loss (progress: 0.10): 0.1827509441440691; Norm Grads: 83.70130359769607
Training Loss (progress: 0.20): 0.16988930443211195; Norm Grads: 79.82948328526471
Training Loss (progress: 0.30): 0.17751042215812282; Norm Grads: 96.39112975928377
Training Loss (progress: 0.40): 0.1788319836194965; Norm Grads: 101.98175475220395
Training Loss (progress: 0.50): 0.16497186681063106; Norm Grads: 79.27225925587604
Training Loss (progress: 0.60): 0.17612529461178011; Norm Grads: 80.90483273081145
Training Loss (progress: 0.70): 0.1580014257877265; Norm Grads: 79.05393472538161
Training Loss (progress: 0.80): 0.16236845577327336; Norm Grads: 74.13635894121207
Training Loss (progress: 0.90): 0.15796317039190605; Norm Grads: 78.90577141397794
Evaluation on validation dataset:
Step 25, mean loss 0.07360569769064372
Step 50, mean loss 0.03567248482951024
Step 75, mean loss 0.04157986984103302
Step 100, mean loss 0.04596775493072495
Step 125, mean loss 0.05414313507304945
Step 150, mean loss 0.06728263661306537
Step 175, mean loss 0.11015809647689753
Step 200, mean loss 0.10959325223789332
Step 225, mean loss 0.12281149384350544
Unrolled forward losses 1.9842830655653398
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.060756022636025364
Step 50, mean loss 0.03465967297348968
Step 75, mean loss 0.03450188923413807
Step 100, mean loss 0.03980826111302629
Step 125, mean loss 0.05267992794242207
Step 150, mean loss 0.06035115725009902
Step 175, mean loss 0.08537420703831614
Step 200, mean loss 0.10114710648024475
Step 225, mean loss 0.132614096099055
Unrolled forward losses 2.1984142689131785
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  7:34:19.631073 

Epoch 5
Starting epoch 5...
Training Loss (progress: 0.00): 0.14226235759860018; Norm Grads: 61.88488207689865
Training Loss (progress: 0.10): 0.13725166825292826; Norm Grads: 50.899575879112454
Training Loss (progress: 0.20): 0.15325505764601077; Norm Grads: 57.20218761104221
Training Loss (progress: 0.30): 0.14721656239952335; Norm Grads: 78.32801664793463
Training Loss (progress: 0.40): 0.1402666585265518; Norm Grads: 70.41311423278486
Training Loss (progress: 0.50): 0.14686405685513296; Norm Grads: 59.55173476599889
Training Loss (progress: 0.60): 0.14515474500010203; Norm Grads: 66.97592336220335
Training Loss (progress: 0.70): 0.13441580729497593; Norm Grads: 68.1980574397303
Training Loss (progress: 0.80): 0.1463645819547071; Norm Grads: 61.7970422852481
Training Loss (progress: 0.90): 0.13493727658798918; Norm Grads: 68.44636814262267
Evaluation on validation dataset:
Step 25, mean loss 0.058768746192414206
Step 50, mean loss 0.02697561484315032
Step 75, mean loss 0.03387577186072875
Step 100, mean loss 0.03813598710423103
Step 125, mean loss 0.04338612331488699
Step 150, mean loss 0.053150948600573185
Step 175, mean loss 0.10077478265041143
Step 200, mean loss 0.08860289102080232
Step 225, mean loss 0.11015533131779015
Unrolled forward losses 1.731969725835253
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.05021371432114161
Step 50, mean loss 0.024728125621488642
Step 75, mean loss 0.02870037922703422
Step 100, mean loss 0.03596632200998201
Step 125, mean loss 0.04346049262006396
Step 150, mean loss 0.046133920892753186
Step 175, mean loss 0.06794136402894027
Step 200, mean loss 0.0944597196087723
Step 225, mean loss 0.11035715537773552
Unrolled forward losses 1.8955305287966846
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  9:08:41.943100 

Epoch 6
Starting epoch 6...
Training Loss (progress: 0.00): 0.12311917905807972; Norm Grads: 71.67218399810636
Training Loss (progress: 0.10): 0.1496857080305953; Norm Grads: 65.95496200780592
Training Loss (progress: 0.20): 0.14418356605370314; Norm Grads: 64.93225959857114
Training Loss (progress: 0.30): 0.15120109098308573; Norm Grads: 72.11285126119651
Training Loss (progress: 0.40): 0.14387779885090304; Norm Grads: 70.37692725212024
Training Loss (progress: 0.50): 0.1393427764781817; Norm Grads: 66.62345412745702
Training Loss (progress: 0.60): 0.13676126349757253; Norm Grads: 71.56512417800785
Training Loss (progress: 0.70): 0.146551397401383; Norm Grads: 73.95988916547897
Training Loss (progress: 0.80): 0.1391136821247178; Norm Grads: 64.66967333679725
Training Loss (progress: 0.90): 0.14063625973114285; Norm Grads: 70.18145922502451
Evaluation on validation dataset:
Step 25, mean loss 0.050808029962165255
Step 50, mean loss 0.025450257384507513
Step 75, mean loss 0.032100777278988364
Step 100, mean loss 0.037460318518897695
Step 125, mean loss 0.04270551660242981
Step 150, mean loss 0.04951713996198435
Step 175, mean loss 0.0923305091382438
Step 200, mean loss 0.08504824337252617
Step 225, mean loss 0.10167900779916186
Unrolled forward losses 1.6176693373512046
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.044258390750765875
Step 50, mean loss 0.023353911589192457
Step 75, mean loss 0.02707975010625469
Step 100, mean loss 0.03330662500467376
Step 125, mean loss 0.04200876906987355
Step 150, mean loss 0.04286475998996814
Step 175, mean loss 0.06653599148302863
Step 200, mean loss 0.09400122422076582
Step 225, mean loss 0.102986458578754
Unrolled forward losses 1.8005272713031841
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  10:43:22.004790 

Epoch 7
Starting epoch 7...
Training Loss (progress: 0.00): 0.14291103655597606; Norm Grads: 76.88267052360317
Training Loss (progress: 0.10): 0.1349290878165619; Norm Grads: 75.00808736896361
Training Loss (progress: 0.20): 0.1331781604259331; Norm Grads: 69.57220734186704
Training Loss (progress: 0.30): 0.1464174795344144; Norm Grads: 76.61053272188022
Training Loss (progress: 0.40): 0.13375764377055685; Norm Grads: 75.8261630312868
Training Loss (progress: 0.50): 0.13281307143868493; Norm Grads: 71.13378360432368
Training Loss (progress: 0.60): 0.13968284031629827; Norm Grads: 72.16396219391144
Training Loss (progress: 0.70): 0.14115314535086698; Norm Grads: 70.84495085132654
Training Loss (progress: 0.80): 0.1474056845074669; Norm Grads: 84.10318990700841
Training Loss (progress: 0.90): 0.13652000892045274; Norm Grads: 63.78081094850117
Evaluation on validation dataset:
Step 25, mean loss 0.050025795763185395
Step 50, mean loss 0.022149005021749105
Step 75, mean loss 0.028687128413416002
Step 100, mean loss 0.03452800524283585
Step 125, mean loss 0.039757460584242
Step 150, mean loss 0.04523099451988636
Step 175, mean loss 0.08884024468565511
Step 200, mean loss 0.08641893578686806
Step 225, mean loss 0.09923636388813442
Unrolled forward losses 1.551958398747642
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.0427513218174051
Step 50, mean loss 0.02104942895016329
Step 75, mean loss 0.025037508332474853
Step 100, mean loss 0.03158471203076728
Step 125, mean loss 0.039414477380059
Step 150, mean loss 0.04043192470995691
Step 175, mean loss 0.06753666537508073
Step 200, mean loss 0.08004321263270991
Step 225, mean loss 0.09795723731446723
Unrolled forward losses 1.6644935499091975
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  12:55:30.659672 

Epoch 8
Starting epoch 8...
Training Loss (progress: 0.00): 0.12990990111962747; Norm Grads: 70.41631278071316
Training Loss (progress: 0.10): 0.13426996833161478; Norm Grads: 72.48964513581066
Training Loss (progress: 0.20): 0.13702700034588072; Norm Grads: 86.32435927709082
Training Loss (progress: 0.30): 0.13001031907927915; Norm Grads: 63.10961003381254
Training Loss (progress: 0.40): 0.14158430460813312; Norm Grads: 65.82991455882937
Training Loss (progress: 0.50): 0.1287374997802544; Norm Grads: 67.69151179958514
Training Loss (progress: 0.60): 0.12937583358326696; Norm Grads: 52.46700687003966
Training Loss (progress: 0.70): 0.12298097286379127; Norm Grads: 67.35974242483083
Training Loss (progress: 0.80): 0.1319617439738903; Norm Grads: 59.398991392867565
Training Loss (progress: 0.90): 0.13103153206338158; Norm Grads: 64.71045277612635
Evaluation on validation dataset:
Step 25, mean loss 0.05573885476917602
Step 50, mean loss 0.028036173115340225
Step 75, mean loss 0.029454874354477634
Step 100, mean loss 0.032656613039366726
Step 125, mean loss 0.037679345945640444
Step 150, mean loss 0.041762570674230004
Step 175, mean loss 0.08935541813959275
Step 200, mean loss 0.08332287542485814
Step 225, mean loss 0.09876513382752405
Unrolled forward losses 1.7374696039810194
Unrolled forward base losses 3.1708552948699085
Epoch 9
Starting epoch 9...
Training Loss (progress: 0.00): 0.13470761498075348; Norm Grads: 72.87508192960695
Training Loss (progress: 0.10): 0.1283379011049621; Norm Grads: 72.56017516854956
Training Loss (progress: 0.20): 0.12645902725985933; Norm Grads: 58.67870244208172
Training Loss (progress: 0.30): 0.1365389821758355; Norm Grads: 72.16465393579166
Training Loss (progress: 0.40): 0.1198928188469083; Norm Grads: 77.20028980886016
Training Loss (progress: 0.50): 0.12922348948340995; Norm Grads: 92.11687596082527
Training Loss (progress: 0.60): 0.12525686270925648; Norm Grads: 72.18912197875694
Training Loss (progress: 0.70): 0.1182897621907685; Norm Grads: 63.07281356590507
Training Loss (progress: 0.80): 0.12386940325989042; Norm Grads: 68.71575126059079
Training Loss (progress: 0.90): 0.11762237525938671; Norm Grads: 61.15004823603303
Evaluation on validation dataset:
Step 25, mean loss 0.04789746047624102
Step 50, mean loss 0.024189608343000074
Step 75, mean loss 0.029425898688358515
Step 100, mean loss 0.030608716824987225
Step 125, mean loss 0.03519313218478242
Step 150, mean loss 0.04105457994451368
Step 175, mean loss 0.09015140938503435
Step 200, mean loss 0.07502614724500334
Step 225, mean loss 0.08689526109908594
Unrolled forward losses 1.5474476333629128
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.04104293366418703
Step 50, mean loss 0.02156455909420993
Step 75, mean loss 0.025150029717780346
Step 100, mean loss 0.030544017836585453
Step 125, mean loss 0.03607760292785791
Step 150, mean loss 0.040065819048903994
Step 175, mean loss 0.060543850962787166
Step 200, mean loss 0.07196720404240592
Step 225, mean loss 0.08772963953861779
Unrolled forward losses 1.7892453689129568
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  16:08:05.630857 

Epoch 10
Starting epoch 10...
Training Loss (progress: 0.00): 0.1152677742639402; Norm Grads: 55.5869244139548
Training Loss (progress: 0.10): 0.10237534263288164; Norm Grads: 50.66011952974148
Training Loss (progress: 0.20): 0.11989649108623213; Norm Grads: 54.41870152391212
Training Loss (progress: 0.30): 0.1206958917715046; Norm Grads: 51.314554064086856
Training Loss (progress: 0.40): 0.11679148350034095; Norm Grads: 61.250615858377046
Training Loss (progress: 0.50): 0.11819115813354587; Norm Grads: 65.70180566278452
Training Loss (progress: 0.60): 0.12258719401169518; Norm Grads: 58.069626265445564
Training Loss (progress: 0.70): 0.10460172474624216; Norm Grads: 50.78345881386806
Training Loss (progress: 0.80): 0.11483754774660215; Norm Grads: 54.651836184768946
Training Loss (progress: 0.90): 0.11819297548165904; Norm Grads: 66.8532966343073
Evaluation on validation dataset:
Step 25, mean loss 0.045369616562311574
Step 50, mean loss 0.020574466962221618
Step 75, mean loss 0.026296062417360593
Step 100, mean loss 0.02793704109207089
Step 125, mean loss 0.03354015684239315
Step 150, mean loss 0.03722323085568273
Step 175, mean loss 0.09070841567517379
Step 200, mean loss 0.07488059410809689
Step 225, mean loss 0.0836668923562089
Unrolled forward losses 1.4863033908251577
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.03763717429888256
Step 50, mean loss 0.018254923475018156
Step 75, mean loss 0.02073533653246984
Step 100, mean loss 0.02602000402166122
Step 125, mean loss 0.032314665470145755
Step 150, mean loss 0.03461967215681416
Step 175, mean loss 0.05911717373588387
Step 200, mean loss 0.07101369045140844
Step 225, mean loss 0.08806287933795641
Unrolled forward losses 1.6165090543235379
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  17:39:55.027987 

Epoch 11
Starting epoch 11...
Training Loss (progress: 0.00): 0.11577030080537315; Norm Grads: 49.48637389186797
Training Loss (progress: 0.10): 0.10652264818307285; Norm Grads: 66.07195557522762
Training Loss (progress: 0.20): 0.11374085386739069; Norm Grads: 52.640112788651265
Training Loss (progress: 0.30): 0.12239984672168175; Norm Grads: 58.93682680246469
Training Loss (progress: 0.40): 0.10834711128054332; Norm Grads: 51.40106994570176
Training Loss (progress: 0.50): 0.12848817780783095; Norm Grads: 59.471375537322615
Training Loss (progress: 0.60): 0.11333839957567413; Norm Grads: 59.379032321191914
Training Loss (progress: 0.70): 0.1078252386652626; Norm Grads: 59.17681943140743
Training Loss (progress: 0.80): 0.10931802768116243; Norm Grads: 53.901177520826415
Training Loss (progress: 0.90): 0.10682007036194774; Norm Grads: 54.32771096681492
Evaluation on validation dataset:
Step 25, mean loss 0.0425940194708746
Step 50, mean loss 0.021150963642142057
Step 75, mean loss 0.025810682308162244
Step 100, mean loss 0.027311150704114308
Step 125, mean loss 0.032295647051638926
Step 150, mean loss 0.0377208986314269
Step 175, mean loss 0.08608608994343678
Step 200, mean loss 0.07406543411037586
Step 225, mean loss 0.0826772086582269
Unrolled forward losses 1.5085155378986996
Unrolled forward base losses 3.1708552948699085
Epoch 12
Starting epoch 12...
Training Loss (progress: 0.00): 0.11017918601863409; Norm Grads: 58.79559171885204
Training Loss (progress: 0.10): 0.11685026639315595; Norm Grads: 59.97605351624687
Training Loss (progress: 0.20): 0.12518556962969046; Norm Grads: 53.9621695721808
Training Loss (progress: 0.30): 0.11145490065405504; Norm Grads: 59.44053395982201
Training Loss (progress: 0.40): 0.11801046532550147; Norm Grads: 66.9818817401756
Training Loss (progress: 0.50): 0.11502377138313347; Norm Grads: 59.87070121115307
Training Loss (progress: 0.60): 0.10697372478000503; Norm Grads: 53.926304055661454
Training Loss (progress: 0.70): 0.10539267417446235; Norm Grads: 60.89326791132959
Training Loss (progress: 0.80): 0.11269684370344368; Norm Grads: 52.824666799984115
Training Loss (progress: 0.90): 0.11074879044105057; Norm Grads: 53.179900684133536
Evaluation on validation dataset:
Step 25, mean loss 0.037588922240116195
Step 50, mean loss 0.019353801145624395
Step 75, mean loss 0.024772837582661175
Step 100, mean loss 0.027874568349681127
Step 125, mean loss 0.033931860586359205
Step 150, mean loss 0.03674657027993078
Step 175, mean loss 0.08189540807703954
Step 200, mean loss 0.07595024167886388
Step 225, mean loss 0.08100411456890635
Unrolled forward losses 1.5170255421046528
Unrolled forward base losses 3.1708552948699085
Epoch 13
Starting epoch 13...
Training Loss (progress: 0.00): 0.11010070322101649; Norm Grads: 60.39339164211052
Training Loss (progress: 0.10): 0.10539382045725464; Norm Grads: 55.48905775365606
Training Loss (progress: 0.20): 0.09398486756529599; Norm Grads: 48.69487243118506
Training Loss (progress: 0.30): 0.12318357503405668; Norm Grads: 67.71705898945068
Training Loss (progress: 0.40): 0.10488220468740674; Norm Grads: 59.90997321221687
Training Loss (progress: 0.50): 0.11552223439534426; Norm Grads: 56.20057657609524
Training Loss (progress: 0.60): 0.11253844836622474; Norm Grads: 70.03900580337607
Training Loss (progress: 0.70): 0.11094075289466351; Norm Grads: 65.86273083161556
Training Loss (progress: 0.80): 0.1149071994114554; Norm Grads: 63.03190994108628
Training Loss (progress: 0.90): 0.10413098005013016; Norm Grads: 55.262806690720794
Evaluation on validation dataset:
Step 25, mean loss 0.0438024925421104
Step 50, mean loss 0.02164230827520964
Step 75, mean loss 0.02538706821819752
Step 100, mean loss 0.027851164045581675
Step 125, mean loss 0.03281929966488925
Step 150, mean loss 0.03751362765695172
Step 175, mean loss 0.08437417595627657
Step 200, mean loss 0.07414544303032093
Step 225, mean loss 0.07961985390411216
Unrolled forward losses 1.565838135634996
Unrolled forward base losses 3.1708552948699085
Epoch 14
Starting epoch 14...
Training Loss (progress: 0.00): 0.10951222407996472; Norm Grads: 65.84149617223618
Training Loss (progress: 0.10): 0.10963722599107485; Norm Grads: 57.39808624191063
Training Loss (progress: 0.20): 0.103963840779289; Norm Grads: 69.66771437007837
Training Loss (progress: 0.30): 0.11223122792931958; Norm Grads: 59.027724399296595
Training Loss (progress: 0.40): 0.11006278735270764; Norm Grads: 60.7134243552058
Training Loss (progress: 0.50): 0.11096791010032903; Norm Grads: 59.94076243125492
Training Loss (progress: 0.60): 0.10164807767534585; Norm Grads: 56.65433781328728
Training Loss (progress: 0.70): 0.11246913007277494; Norm Grads: 64.52378573914591
Training Loss (progress: 0.80): 0.10242234762744032; Norm Grads: 55.79938997164299
Training Loss (progress: 0.90): 0.11319579458249564; Norm Grads: 67.38390225754112
Evaluation on validation dataset:
Step 25, mean loss 0.04335597350536181
Step 50, mean loss 0.020057981114430523
Step 75, mean loss 0.024269892729091143
Step 100, mean loss 0.026389831389211887
Step 125, mean loss 0.03123474130525224
Step 150, mean loss 0.03666602163289607
Step 175, mean loss 0.08649977372022755
Step 200, mean loss 0.07136228345529613
Step 225, mean loss 0.07864819587528149
Unrolled forward losses 1.4766611288675424
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.036182582292811
Step 50, mean loss 0.017591335742095673
Step 75, mean loss 0.019974107197906467
Step 100, mean loss 0.02523193296780237
Step 125, mean loss 0.029689524575783506
Step 150, mean loss 0.032745536685056406
Step 175, mean loss 0.05590164782012307
Step 200, mean loss 0.06860598141097296
Step 225, mean loss 0.08181654535514472
Unrolled forward losses 1.5457196488617588
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  23:49:46.063500 

Epoch 15
Starting epoch 15...
Training Loss (progress: 0.00): 0.11627545432348788; Norm Grads: 46.209442099592636
Training Loss (progress: 0.10): 0.11592781226046026; Norm Grads: 59.548653356687005
Training Loss (progress: 0.20): 0.10055102604930839; Norm Grads: 46.23086427722983
Training Loss (progress: 0.30): 0.12037628950891713; Norm Grads: 50.33096745722727
Training Loss (progress: 0.40): 0.1078467916020525; Norm Grads: 46.424756896765956
Training Loss (progress: 0.50): 0.12159671760622552; Norm Grads: 50.180688383167634
Training Loss (progress: 0.60): 0.10170879853763243; Norm Grads: 50.84564804187656
Training Loss (progress: 0.70): 0.10051890261544534; Norm Grads: 44.84315190030446
Training Loss (progress: 0.80): 0.10354926312886452; Norm Grads: 61.374787191308556
Training Loss (progress: 0.90): 0.10342593636783565; Norm Grads: 42.17755410661906
Evaluation on validation dataset:
Step 25, mean loss 0.03689591345518302
Step 50, mean loss 0.018350973177439202
Step 75, mean loss 0.02339424089028527
Step 100, mean loss 0.026393996496777393
Step 125, mean loss 0.031031693837678134
Step 150, mean loss 0.035157384923306
Step 175, mean loss 0.08656247102133881
Step 200, mean loss 0.07070579887336237
Step 225, mean loss 0.07626994663899152
Unrolled forward losses 1.427483068164849
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.030802013568137408
Step 50, mean loss 0.016371756485485056
Step 75, mean loss 0.019099190367763516
Step 100, mean loss 0.024730816912728985
Step 125, mean loss 0.0301126898592347
Step 150, mean loss 0.03204445116168007
Step 175, mean loss 0.054883860685974464
Step 200, mean loss 0.0675511019230324
Step 225, mean loss 0.0821867482384808
Unrolled forward losses 1.5229106501815481
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  1 day, 1:23:52.123066 

Epoch 16
Starting epoch 16...
Training Loss (progress: 0.00): 0.11333301322452989; Norm Grads: 48.11993437917952
Training Loss (progress: 0.10): 0.11173131055592231; Norm Grads: 53.161603575439266
Training Loss (progress: 0.20): 0.10632533978536642; Norm Grads: 49.916758210307016
Training Loss (progress: 0.30): 0.09811155148480587; Norm Grads: 49.73312118855785
Training Loss (progress: 0.40): 0.10059669931598203; Norm Grads: 54.70104768988318
Training Loss (progress: 0.50): 0.10103812971434421; Norm Grads: 48.242567320881726
Training Loss (progress: 0.60): 0.09910817079973466; Norm Grads: 54.61910273738601
Training Loss (progress: 0.70): 0.10918716020007489; Norm Grads: 56.078234003791195
Training Loss (progress: 0.80): 0.11544523101409802; Norm Grads: 50.347070198193975
Training Loss (progress: 0.90): 0.11666025443520991; Norm Grads: 47.573253619990254
Evaluation on validation dataset:
Step 25, mean loss 0.035143857525604366
Step 50, mean loss 0.018075155879217257
Step 75, mean loss 0.02300490132018148
Step 100, mean loss 0.02568061764327254
Step 125, mean loss 0.03115362192310118
Step 150, mean loss 0.03532042692005729
Step 175, mean loss 0.08714853063322296
Step 200, mean loss 0.07149889633331022
Step 225, mean loss 0.07671503444126748
Unrolled forward losses 1.4167482930728943
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.030007820436068117
Step 50, mean loss 0.016231301349790297
Step 75, mean loss 0.0188945743935817
Step 100, mean loss 0.024526319519287042
Step 125, mean loss 0.029889379675124148
Step 150, mean loss 0.031932153817056275
Step 175, mean loss 0.055506669988403826
Step 200, mean loss 0.06849426937205177
Step 225, mean loss 0.08113317049875837
Unrolled forward losses 1.5210653454860783
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  1 day, 2:58:20.157953 

Epoch 17
Starting epoch 17...
Training Loss (progress: 0.00): 0.10042356306196508; Norm Grads: 44.30508767699479
Training Loss (progress: 0.10): 0.1071528374839736; Norm Grads: 50.20614797103408
Training Loss (progress: 0.20): 0.09400215249082039; Norm Grads: 43.76757003178826
Training Loss (progress: 0.30): 0.09787418377249044; Norm Grads: 55.28522242488355
Training Loss (progress: 0.40): 0.11170503033454036; Norm Grads: 52.916873312004604
Training Loss (progress: 0.50): 0.10977408522539499; Norm Grads: 46.63540918994749
Training Loss (progress: 0.60): 0.10497455179320529; Norm Grads: 51.87309622355741
Training Loss (progress: 0.70): 0.10531654488278228; Norm Grads: 50.64913508198386
Training Loss (progress: 0.80): 0.09509103249070976; Norm Grads: 46.36694730302723
Training Loss (progress: 0.90): 0.11173177215483385; Norm Grads: 51.45202771459223
Evaluation on validation dataset:
Step 25, mean loss 0.03563970819024576
Step 50, mean loss 0.018197517439806486
Step 75, mean loss 0.022655451046764835
Step 100, mean loss 0.025522653952564506
Step 125, mean loss 0.030488628981301103
Step 150, mean loss 0.03514347079891635
Step 175, mean loss 0.08753164248239555
Step 200, mean loss 0.07104576273525823
Step 225, mean loss 0.07688148471946173
Unrolled forward losses 1.4220961474857976
Unrolled forward base losses 3.1708552948699085
Epoch 18
Starting epoch 18...
Training Loss (progress: 0.00): 0.10765749285108689; Norm Grads: 42.314313570149075
Training Loss (progress: 0.10): 0.11207768278521112; Norm Grads: 50.7115457127046
Training Loss (progress: 0.20): 0.1001440454339208; Norm Grads: 46.80423869032833
Training Loss (progress: 0.30): 0.11437337378340219; Norm Grads: 44.81929321607086
Training Loss (progress: 0.40): 0.11021390027904239; Norm Grads: 47.50338295503834
Training Loss (progress: 0.50): 0.10273747772481384; Norm Grads: 52.00162125721468
Training Loss (progress: 0.60): 0.10262875033728025; Norm Grads: 53.2512941909334
Training Loss (progress: 0.70): 0.10007824732530948; Norm Grads: 48.698423743869874
Training Loss (progress: 0.80): 0.09863622256958164; Norm Grads: 48.94556000634801
Training Loss (progress: 0.90): 0.10588787795268241; Norm Grads: 51.00224403457656
Evaluation on validation dataset:
Step 25, mean loss 0.035095485750430595
Step 50, mean loss 0.017717547229562487
Step 75, mean loss 0.02249514969190448
Step 100, mean loss 0.0252261226516458
Step 125, mean loss 0.030395193641171346
Step 150, mean loss 0.03466934033916451
Step 175, mean loss 0.08797340479443327
Step 200, mean loss 0.07149791188450982
Step 225, mean loss 0.07673951599179607
Unrolled forward losses 1.4055811273263326
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.029524112270870276
Step 50, mean loss 0.015794501229678085
Step 75, mean loss 0.01855878739203353
Step 100, mean loss 0.024098985586125457
Step 125, mean loss 0.029235478578005604
Step 150, mean loss 0.03190335877818299
Step 175, mean loss 0.05594028719081594
Step 200, mean loss 0.06766070301025748
Step 225, mean loss 0.08257413104942393
Unrolled forward losses 1.4992739586898798
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  1 day, 6:08:15.184875 

Epoch 19
Starting epoch 19...
Training Loss (progress: 0.00): 0.1050103134531785; Norm Grads: 50.90652503258951
Training Loss (progress: 0.10): 0.10590698721762434; Norm Grads: 50.87374970459705
Training Loss (progress: 0.20): 0.10316784351168627; Norm Grads: 51.73052482518779
Training Loss (progress: 0.30): 0.08779417394058558; Norm Grads: 46.56611402220426
Training Loss (progress: 0.40): 0.10641389037545829; Norm Grads: 54.62240916942067
Training Loss (progress: 0.50): 0.09608544815934224; Norm Grads: 47.2171518122871
Training Loss (progress: 0.60): 0.09703610132016982; Norm Grads: 52.215387865275964
Training Loss (progress: 0.70): 0.1114915384055478; Norm Grads: 51.99451175064503
Training Loss (progress: 0.80): 0.12349481184080528; Norm Grads: 61.224114915729615
Training Loss (progress: 0.90): 0.10078428926738119; Norm Grads: 51.9066800829078
Evaluation on validation dataset:
Step 25, mean loss 0.03416948313697962
Step 50, mean loss 0.018261419293125424
Step 75, mean loss 0.023258611273694066
Step 100, mean loss 0.02512591907032132
Step 125, mean loss 0.03077294836648458
Step 150, mean loss 0.03525669051889582
Step 175, mean loss 0.09003717910743793
Step 200, mean loss 0.07254695161700175
Step 225, mean loss 0.07687645603169463
Unrolled forward losses 1.4264394298219283
Unrolled forward base losses 3.1708552948699085
Epoch 20
Starting epoch 20...
Training Loss (progress: 0.00): 0.09846413289444042; Norm Grads: 47.05193053936413
Training Loss (progress: 0.10): 0.11144433039454962; Norm Grads: 49.79507405298287
Training Loss (progress: 0.20): 0.10226957327075833; Norm Grads: 48.13853963110195
Training Loss (progress: 0.30): 0.10122224510924105; Norm Grads: 51.10366662338686
Training Loss (progress: 0.40): 0.10047815408224677; Norm Grads: 50.849197740804215
Training Loss (progress: 0.50): 0.10145538384052775; Norm Grads: 53.419910724351226
Training Loss (progress: 0.60): 0.10125260124369187; Norm Grads: 42.573549865741285
Training Loss (progress: 0.70): 0.10223795593270137; Norm Grads: 48.06673964589471
Training Loss (progress: 0.80): 0.1081799850959503; Norm Grads: 48.73358930764264
Training Loss (progress: 0.90): 0.10680805452626922; Norm Grads: 50.9081261106929
Evaluation on validation dataset:
Step 25, mean loss 0.03417397387520723
Step 50, mean loss 0.01787571776306975
Step 75, mean loss 0.022478790779083498
Step 100, mean loss 0.0245238977609413
Step 125, mean loss 0.029979625874499167
Step 150, mean loss 0.03460791465265177
Step 175, mean loss 0.08768812416339916
Step 200, mean loss 0.07092956113076085
Step 225, mean loss 0.07422002711843746
Unrolled forward losses 1.4345811457990916
Unrolled forward base losses 3.1708552948699085
Epoch 21
Starting epoch 21...
Training Loss (progress: 0.00): 0.10406978690388383; Norm Grads: 57.6264853383268
Training Loss (progress: 0.10): 0.09816974776961213; Norm Grads: 49.82440377522911
Training Loss (progress: 0.20): 0.10915740840687203; Norm Grads: 52.36621590476473
Training Loss (progress: 0.30): 0.11015961142314351; Norm Grads: 49.46183120947532
Training Loss (progress: 0.40): 0.11178276976947968; Norm Grads: 61.75144686419148
Training Loss (progress: 0.50): 0.10125939063204767; Norm Grads: 50.17767449624426
Training Loss (progress: 0.60): 0.10043063030082858; Norm Grads: 52.01893920770014
Training Loss (progress: 0.70): 0.10349514609837517; Norm Grads: 49.02868587999198
Training Loss (progress: 0.80): 0.10672848651277267; Norm Grads: 45.04437989309076
Training Loss (progress: 0.90): 0.09671642276950877; Norm Grads: 53.99737537411193
Evaluation on validation dataset:
Step 25, mean loss 0.03521105451216449
Step 50, mean loss 0.017892846460166104
Step 75, mean loss 0.022403561509905745
Step 100, mean loss 0.024983623679045684
Step 125, mean loss 0.030086843590431418
Step 150, mean loss 0.03436104808798554
Step 175, mean loss 0.08457831215586384
Step 200, mean loss 0.06881188369084257
Step 225, mean loss 0.07454104835015535
Unrolled forward losses 1.3991466773882952
Unrolled forward base losses 3.1708552948699085
Evaluation on test dataset:
Step 25, mean loss 0.029503715076666482
Step 50, mean loss 0.01567914132926818
Step 75, mean loss 0.01838687175379091
Step 100, mean loss 0.023583955873009325
Step 125, mean loss 0.02858487896711935
Step 150, mean loss 0.031241532921657045
Step 175, mean loss 0.054014462380060954
Step 200, mean loss 0.06554834346150673
Step 225, mean loss 0.07941564140154193
Unrolled forward losses 1.5084704777587419
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n6_tw25_unrolling2_time121713.pt
Training time:  1 day, 10:54:37.501883 

Epoch 22
Starting epoch 22...
Training Loss (progress: 0.00): 0.11332964353078574; Norm Grads: 53.24708053964033
Training Loss (progress: 0.10): 0.10481127956678242; Norm Grads: 50.17502875677685
Training Loss (progress: 0.20): 0.10868053347274043; Norm Grads: 52.06206881451251
Training Loss (progress: 0.30): 0.10421590751274332; Norm Grads: 42.10489612455202
Training Loss (progress: 0.40): 0.10311218533801993; Norm Grads: 49.527132691372984
Training Loss (progress: 0.50): 0.10959434103243898; Norm Grads: 55.52019732625404
Training Loss (progress: 0.60): 0.09809689753290737; Norm Grads: 57.46311332089009
Training Loss (progress: 0.70): 0.10286259278416009; Norm Grads: 47.81704767811357
Training Loss (progress: 0.80): 0.09793533466042713; Norm Grads: 51.51568309733736
Training Loss (progress: 0.90): 0.10345587186524033; Norm Grads: 50.7065383301198
Evaluation on validation dataset:
Step 25, mean loss 0.03218440377609649
Step 50, mean loss 0.01771385584803392
Step 75, mean loss 0.022673187425419006
Step 100, mean loss 0.025388654015614694
Step 125, mean loss 0.02971423857234797
Step 150, mean loss 0.033690814572147884
Step 175, mean loss 0.08487301466151648
Step 200, mean loss 0.06810989561400438
Step 225, mean loss 0.074236330256549
Unrolled forward losses 1.4013124712519915
Unrolled forward base losses 3.1708552948699085
Epoch 23
Starting epoch 23...
Training Loss (progress: 0.00): 0.10367269903226008; Norm Grads: 46.94424976118077
Training Loss (progress: 0.10): 0.09929208022716668; Norm Grads: 49.33648893125619
Training Loss (progress: 0.20): 0.10853557797036771; Norm Grads: 48.13906869242694
Training Loss (progress: 0.30): 0.10508911829289228; Norm Grads: 51.13645039365704
Training Loss (progress: 0.40): 0.09412131777898873; Norm Grads: 47.46013101817891
Training Loss (progress: 0.50): 0.10905669130632123; Norm Grads: 58.12745384931257
Training Loss (progress: 0.60): 0.09823302011685583; Norm Grads: 48.401251476083004
Training Loss (progress: 0.70): 0.10228517556135273; Norm Grads: 47.09306886965026
Training Loss (progress: 0.80): 0.1052195832275545; Norm Grads: 48.44671947465174
Training Loss (progress: 0.90): 0.09803188758262837; Norm Grads: 44.11493123831238
Evaluation on validation dataset:
Step 25, mean loss 0.03189547310279815
Step 50, mean loss 0.017565523983951965
Step 75, mean loss 0.02250353343746521
Step 100, mean loss 0.02435551998501393
Step 125, mean loss 0.029087559437949256
Step 150, mean loss 0.03366322534169047
Step 175, mean loss 0.08745426638440382
Step 200, mean loss 0.06929264959467642
Step 225, mean loss 0.07330267982656757
Unrolled forward losses 1.4311934903920127
Unrolled forward base losses 3.1708552948699085
Epoch 24
Starting epoch 24...
Training Loss (progress: 0.00): 0.10021103612212014; Norm Grads: 55.55852516974027
Training Loss (progress: 0.10): 0.09413361696652504; Norm Grads: 45.00481583217899
Training Loss (progress: 0.20): 0.09466126671361136; Norm Grads: 46.863866732137105
Training Loss (progress: 0.30): 0.10024156773414765; Norm Grads: 50.69714689085734
Training Loss (progress: 0.40): 0.10925319171875028; Norm Grads: 52.28343628706508
Training Loss (progress: 0.50): 0.10135555199940956; Norm Grads: 50.58107885967522
Training Loss (progress: 0.60): 0.10777090159968085; Norm Grads: 54.52425070724562
Training Loss (progress: 0.70): 0.10845621058885811; Norm Grads: 52.34462496256267
Training Loss (progress: 0.80): 0.10143669812909728; Norm Grads: 49.111926581940295
Training Loss (progress: 0.90): 0.09814993352704804; Norm Grads: 56.46119754405241
Evaluation on validation dataset:
Step 25, mean loss 0.032070181209962474
Step 50, mean loss 0.017401759218911985
Step 75, mean loss 0.021843087744164963
Step 100, mean loss 0.02427048132749705
Step 125, mean loss 0.02888274392656493
Step 150, mean loss 0.03325445715758968
Step 175, mean loss 0.08506578169489555
Step 200, mean loss 0.06891033475663759
Step 225, mean loss 0.0725328086362362
Unrolled forward losses 1.4138777937240072
Unrolled forward base losses 3.1708552948699085
Test loss: 1.5084704777587419
Training time (until epoch 21):  {datetime.timedelta(days=1, seconds=39277, microseconds=501883)}
