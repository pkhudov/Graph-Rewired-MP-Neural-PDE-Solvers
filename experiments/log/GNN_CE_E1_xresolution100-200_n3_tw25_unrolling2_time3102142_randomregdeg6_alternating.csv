Training on dataset data/CE_train_E1.h5
cuda:0
models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Number of parameters: 1031645
Training started at: 2025-03-10 21:42:47
Epoch 0
Starting epoch 0...
Generated custom edges
Training Loss (progress: 0.00): 1.3126354846073554; Norm Grads: 34.25445965542721
Training Loss (progress: 0.10): 0.2293698670882614; Norm Grads: 175.36987583350515
Training Loss (progress: 0.20): 0.1894638274860841; Norm Grads: 200.4840286280685
Training Loss (progress: 0.30): 0.17327391579684798; Norm Grads: 176.59938643234315
Training Loss (progress: 0.40): 0.14377350826006824; Norm Grads: 205.42321155018098
Training Loss (progress: 0.50): 0.13464295154951736; Norm Grads: 151.9776525461958
Training Loss (progress: 0.60): 0.13289843045388558; Norm Grads: 178.48833867393878
Training Loss (progress: 0.70): 0.1339550943523077; Norm Grads: 192.4877549006967
Training Loss (progress: 0.80): 0.11008656479212692; Norm Grads: 137.99804875977523
Training Loss (progress: 0.90): 0.1174902597074054; Norm Grads: 164.8143405521409
Evaluation on validation dataset:
Step 25, mean loss 0.09465064336169093
Step 50, mean loss 0.10842012876322299
Step 75, mean loss 0.1148714095523283
Step 100, mean loss 0.1035809802851602
Step 125, mean loss 0.15087467784490416
Step 150, mean loss 0.16009795994305362
Step 175, mean loss 0.5236063103411398
Step 200, mean loss 0.2656917081152461
Step 225, mean loss 0.3644535345862472
Unrolled forward losses 22.557069410996245
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.07697040909385258
Step 50, mean loss 0.0800093674008242
Step 75, mean loss 0.08182863601785315
Step 100, mean loss 0.10493952811006642
Step 125, mean loss 0.13534213903629302
Step 150, mean loss 0.17212669245901238
Step 175, mean loss 0.751401206039922
Step 200, mean loss 0.2658174256255777
Step 225, mean loss 0.23426236113766125
Unrolled forward losses 26.04429797553877
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  1:06:02.923392 

Epoch 1
Starting epoch 1...
Training Loss (progress: 0.00): 0.2506633294225252; Norm Grads: 142.0999996325526
Training Loss (progress: 0.10): 0.24622769819686058; Norm Grads: 91.79837632536365
Training Loss (progress: 0.20): 0.24098589187187305; Norm Grads: 107.53376887675246
Training Loss (progress: 0.30): 0.20317871829389378; Norm Grads: 134.99257972914725
Training Loss (progress: 0.40): 0.2113018171697938; Norm Grads: 115.63769144835783
Training Loss (progress: 0.50): 0.20602290497572126; Norm Grads: 104.47418787995115
Training Loss (progress: 0.60): 0.20192272635884662; Norm Grads: 106.76568979335246
Training Loss (progress: 0.70): 0.17553300157655086; Norm Grads: 84.96580832437103
Training Loss (progress: 0.80): 0.1703405364429597; Norm Grads: 90.13915821901567
Training Loss (progress: 0.90): 0.1791070397004281; Norm Grads: 89.9623449481926
Evaluation on validation dataset:
Step 25, mean loss 0.09168603984760529
Step 50, mean loss 0.09466506530998137
Step 75, mean loss 0.07506513709482905
Step 100, mean loss 0.08072023582799286
Step 125, mean loss 0.09947634313083117
Step 150, mean loss 0.12605759289567647
Step 175, mean loss 0.19798724896350894
Step 200, mean loss 0.19773430828591948
Step 225, mean loss 0.17444395647565714
Unrolled forward losses 4.1814507004187895
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.08140187049910061
Step 50, mean loss 0.06543804353097116
Step 75, mean loss 0.06615441704714241
Step 100, mean loss 0.07507970497516904
Step 125, mean loss 0.10139880838877474
Step 150, mean loss 0.11146932510039087
Step 175, mean loss 0.1948974828103175
Step 200, mean loss 0.16035873881955479
Step 225, mean loss 0.16282348177057876
Unrolled forward losses 4.4260354027229205
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  2:18:45.704330 

Epoch 2
Starting epoch 2...
Training Loss (progress: 0.00): 0.2514139820488194; Norm Grads: 61.0747256424089
Training Loss (progress: 0.10): 0.2520698708796882; Norm Grads: 92.93167577931625
Training Loss (progress: 0.20): 0.2312521850353151; Norm Grads: 82.57419697210308
Training Loss (progress: 0.30): 0.24520961298592397; Norm Grads: 79.37434729522755
Training Loss (progress: 0.40): 0.25580579047216473; Norm Grads: 81.0725056618121
Training Loss (progress: 0.50): 0.2335399480152666; Norm Grads: 86.04901296947699
Training Loss (progress: 0.60): 0.20773920077442717; Norm Grads: 64.95875161681411
Training Loss (progress: 0.70): 0.21680860778034233; Norm Grads: 91.81715981839149
Training Loss (progress: 0.80): 0.2053543350940426; Norm Grads: 73.10396714343628
Training Loss (progress: 0.90): 0.1944741317399612; Norm Grads: 78.93973598497928
Evaluation on validation dataset:
Step 25, mean loss 0.07698327450848377
Step 50, mean loss 0.06336215560850482
Step 75, mean loss 0.05444775530316484
Step 100, mean loss 0.05002643255962309
Step 125, mean loss 0.062395792565815296
Step 150, mean loss 0.07229595773254771
Step 175, mean loss 0.15685693267719097
Step 200, mean loss 0.12718404771345193
Step 225, mean loss 0.1310995822097527
Unrolled forward losses 2.9970145254483667
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.0706574706216342
Step 50, mean loss 0.04901939667620854
Step 75, mean loss 0.04771334800283317
Step 100, mean loss 0.04852094768138982
Step 125, mean loss 0.06295621504685774
Step 150, mean loss 0.06740799162766348
Step 175, mean loss 0.13167540216530227
Step 200, mean loss 0.11068825705927152
Step 225, mean loss 0.10969988197458216
Unrolled forward losses 2.9134161822056424
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  3:33:31.968677 

Epoch 3
Starting epoch 3...
Training Loss (progress: 0.00): 0.2225497877084261; Norm Grads: 91.89757541687138
Training Loss (progress: 0.10): 0.2129374694909327; Norm Grads: 77.68903024668634
Training Loss (progress: 0.20): 0.18949947631027084; Norm Grads: 77.86105587341336
Training Loss (progress: 0.30): 0.1901096375925425; Norm Grads: 85.27988158947718
Training Loss (progress: 0.40): 0.22316759468104777; Norm Grads: 82.50903125045427
Training Loss (progress: 0.50): 0.21416290867029614; Norm Grads: 87.71231421151421
Training Loss (progress: 0.60): 0.2015439235008018; Norm Grads: 87.13185298645134
Training Loss (progress: 0.70): 0.20429783027809142; Norm Grads: 81.22304265197413
Training Loss (progress: 0.80): 0.21125685619929085; Norm Grads: 76.82371969780941
Training Loss (progress: 0.90): 0.1947024324565828; Norm Grads: 92.30583583087216
Evaluation on validation dataset:
Step 25, mean loss 0.06452975961397486
Step 50, mean loss 0.06048322346428223
Step 75, mean loss 0.05522941358289508
Step 100, mean loss 0.05124229730214079
Step 125, mean loss 0.06351338722552874
Step 150, mean loss 0.06736809045620687
Step 175, mean loss 0.14062363548448625
Step 200, mean loss 0.11634024911129291
Step 225, mean loss 0.12750463026054004
Unrolled forward losses 2.6072749518867586
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.05517346744942573
Step 50, mean loss 0.046483677091914574
Step 75, mean loss 0.051240574605351696
Step 100, mean loss 0.05116235431494952
Step 125, mean loss 0.057876539650304
Step 150, mean loss 0.07613649062413513
Step 175, mean loss 0.13300362596715112
Step 200, mean loss 0.11485404812271047
Step 225, mean loss 0.11377346962846879
Unrolled forward losses 2.7117190762826375
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  4:48:18.615627 

Epoch 4
Starting epoch 4...
Training Loss (progress: 0.00): 0.1881644528742059; Norm Grads: 91.2465541830708
Training Loss (progress: 0.10): 0.18843387078097026; Norm Grads: 93.76038569610043
Training Loss (progress: 0.20): 0.18867883096271512; Norm Grads: 90.78190631972593
Training Loss (progress: 0.30): 0.20149323981302286; Norm Grads: 89.19905417479238
Training Loss (progress: 0.40): 0.17313034715336018; Norm Grads: 82.06617680523038
Training Loss (progress: 0.50): 0.16728227000320436; Norm Grads: 98.08979060569985
Training Loss (progress: 0.60): 0.170103772886527; Norm Grads: 100.30146487150179
Training Loss (progress: 0.70): 0.19192400878052632; Norm Grads: 84.74868445935121
Training Loss (progress: 0.80): 0.19353077564244162; Norm Grads: 88.25319281506313
Training Loss (progress: 0.90): 0.1709996313634593; Norm Grads: 81.67521938615516
Evaluation on validation dataset:
Step 25, mean loss 0.05822357080339999
Step 50, mean loss 0.03880927045949142
Step 75, mean loss 0.048688654941727624
Step 100, mean loss 0.046953288852375034
Step 125, mean loss 0.05466241116332206
Step 150, mean loss 0.06893314927803233
Step 175, mean loss 0.09173307099485634
Step 200, mean loss 0.1104161303330349
Step 225, mean loss 0.11473635647132369
Unrolled forward losses 2.2735001297536543
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.0509192961267568
Step 50, mean loss 0.03517144741447132
Step 75, mean loss 0.04068869694423565
Step 100, mean loss 0.041920884597680674
Step 125, mean loss 0.050558853320644484
Step 150, mean loss 0.0566277736738602
Step 175, mean loss 0.08862195302358497
Step 200, mean loss 0.08556518704302818
Step 225, mean loss 0.10180093887439066
Unrolled forward losses 2.283288268968887
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  6:02:46.159980 

Epoch 5
Starting epoch 5...
Training Loss (progress: 0.00): 0.16304969821659598; Norm Grads: 66.7031309488145
Training Loss (progress: 0.10): 0.161498597729198; Norm Grads: 70.0763872689482
Training Loss (progress: 0.20): 0.15667321281331165; Norm Grads: 73.92582754397462
Training Loss (progress: 0.30): 0.15668180283944277; Norm Grads: 84.33836751527681
Training Loss (progress: 0.40): 0.15739701814478288; Norm Grads: 68.57231660769176
Training Loss (progress: 0.50): 0.14908486054931921; Norm Grads: 72.83426481773806
Training Loss (progress: 0.60): 0.157330441800636; Norm Grads: 65.89625746942941
Training Loss (progress: 0.70): 0.16989676222642466; Norm Grads: 78.88808690201233
Training Loss (progress: 0.80): 0.1483573351430892; Norm Grads: 72.01753881254082
Training Loss (progress: 0.90): 0.1542538844309896; Norm Grads: 66.37287346000807
Evaluation on validation dataset:
Step 25, mean loss 0.050209831515154506
Step 50, mean loss 0.03473066546052406
Step 75, mean loss 0.03514402387206762
Step 100, mean loss 0.03361265552905598
Step 125, mean loss 0.043679473067765734
Step 150, mean loss 0.0510425246679144
Step 175, mean loss 0.08536765553795442
Step 200, mean loss 0.08847107096806235
Step 225, mean loss 0.0982723833189047
Unrolled forward losses 1.787451023333665
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.04164355084199939
Step 50, mean loss 0.0275037833640233
Step 75, mean loss 0.03188299903613503
Step 100, mean loss 0.033078251429279906
Step 125, mean loss 0.040159794324533205
Step 150, mean loss 0.051279621820388505
Step 175, mean loss 0.08186971312725547
Step 200, mean loss 0.08005962146542359
Step 225, mean loss 0.08965517681640196
Unrolled forward losses 1.918785068797153
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  7:12:30.588316 

Epoch 6
Starting epoch 6...
Training Loss (progress: 0.00): 0.1471482846458203; Norm Grads: 72.34467212935364
Training Loss (progress: 0.10): 0.15905670877701705; Norm Grads: 77.00032982706709
Training Loss (progress: 0.20): 0.1404915066745288; Norm Grads: 57.23346353067759
Training Loss (progress: 0.30): 0.1607410914431015; Norm Grads: 65.86145964410944
Training Loss (progress: 0.40): 0.1599844701652853; Norm Grads: 69.1495679136287
Training Loss (progress: 0.50): 0.14071051062908668; Norm Grads: 74.63655657586162
Training Loss (progress: 0.60): 0.16927501477522397; Norm Grads: 72.67744996483084
Training Loss (progress: 0.70): 0.13566558745156765; Norm Grads: 79.06930745441606
Training Loss (progress: 0.80): 0.13359889877914105; Norm Grads: 63.39772599018046
Training Loss (progress: 0.90): 0.15061864397879113; Norm Grads: 82.40155834838984
Evaluation on validation dataset:
Step 25, mean loss 0.047285268894431125
Step 50, mean loss 0.03095870981101375
Step 75, mean loss 0.030267608567617878
Step 100, mean loss 0.03226900330829169
Step 125, mean loss 0.040611761099459756
Step 150, mean loss 0.04589266526148496
Step 175, mean loss 0.09615905832355925
Step 200, mean loss 0.07933486177788202
Step 225, mean loss 0.08612392960044957
Unrolled forward losses 1.7560568264831065
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.03770258968178786
Step 50, mean loss 0.023791971373777823
Step 75, mean loss 0.027287523594049026
Step 100, mean loss 0.02858452762349256
Step 125, mean loss 0.0359069170219008
Step 150, mean loss 0.04303178370182395
Step 175, mean loss 0.0794906583378599
Step 200, mean loss 0.07004355735147383
Step 225, mean loss 0.08140118241061164
Unrolled forward losses 1.7008846422003328
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  8:26:07.990205 

Epoch 7
Starting epoch 7...
Training Loss (progress: 0.00): 0.1406397617150616; Norm Grads: 70.80148911704917
Training Loss (progress: 0.10): 0.15503275543288003; Norm Grads: 82.17275784990882
Training Loss (progress: 0.20): 0.13936532136045288; Norm Grads: 76.61866205958265
Training Loss (progress: 0.30): 0.1461823874259337; Norm Grads: 78.74143426526459
Training Loss (progress: 0.40): 0.1383159596224084; Norm Grads: 82.60672086286027
Training Loss (progress: 0.50): 0.131239870859704; Norm Grads: 60.08936846587388
Training Loss (progress: 0.60): 0.1363414036938041; Norm Grads: 62.07825124521081
Training Loss (progress: 0.70): 0.16167628728135697; Norm Grads: 70.65146592865659
Training Loss (progress: 0.80): 0.1406745366288568; Norm Grads: 66.21086692041072
Training Loss (progress: 0.90): 0.14178528104020263; Norm Grads: 65.92803188239215
Evaluation on validation dataset:
Step 25, mean loss 0.03700633516287698
Step 50, mean loss 0.02812951733759304
Step 75, mean loss 0.026618596264644776
Step 100, mean loss 0.026542755750788308
Step 125, mean loss 0.03624397768810965
Step 150, mean loss 0.04279956332553973
Step 175, mean loss 0.08232098250784223
Step 200, mean loss 0.074964564751697
Step 225, mean loss 0.08838068650224554
Unrolled forward losses 1.6865057065762392
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.029838284986869922
Step 50, mean loss 0.023258942743735994
Step 75, mean loss 0.02543514255158267
Step 100, mean loss 0.026457424014856033
Step 125, mean loss 0.03243835959993589
Step 150, mean loss 0.04067774432349627
Step 175, mean loss 0.07999793091232674
Step 200, mean loss 0.06917020560713677
Step 225, mean loss 0.08330092163335727
Unrolled forward losses 1.6776593150812116
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  9:40:40.416291 

Epoch 8
Starting epoch 8...
Training Loss (progress: 0.00): 0.1409750756952289; Norm Grads: 80.39011902501547
Training Loss (progress: 0.10): 0.14105005542429866; Norm Grads: 69.06457486875004
Training Loss (progress: 0.20): 0.14084375664034038; Norm Grads: 64.1057340935719
Training Loss (progress: 0.30): 0.14271696620297386; Norm Grads: 83.35246744382955
Training Loss (progress: 0.40): 0.14745189431544892; Norm Grads: 97.97648037806701
Training Loss (progress: 0.50): 0.14051173769005315; Norm Grads: 85.68481996566011
Training Loss (progress: 0.60): 0.13302485167306843; Norm Grads: 69.16307834866252
Training Loss (progress: 0.70): 0.13197347026560005; Norm Grads: 73.01801966407199
Training Loss (progress: 0.80): 0.12860192382314808; Norm Grads: 71.34910103898781
Training Loss (progress: 0.90): 0.13434192738324705; Norm Grads: 67.52721324219304
Evaluation on validation dataset:
Step 25, mean loss 0.032151646921543986
Step 50, mean loss 0.02238432517475575
Step 75, mean loss 0.025794591561188344
Step 100, mean loss 0.026394639396048844
Step 125, mean loss 0.03538095837687236
Step 150, mean loss 0.04262352400769844
Step 175, mean loss 0.08639715785703647
Step 200, mean loss 0.07060197604687606
Step 225, mean loss 0.07755092339830608
Unrolled forward losses 1.4801932790818242
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.028807760815834655
Step 50, mean loss 0.020692208377347452
Step 75, mean loss 0.025680501412779208
Step 100, mean loss 0.025148476464743507
Step 125, mean loss 0.03255301451092728
Step 150, mean loss 0.03883510746365283
Step 175, mean loss 0.07224760303635519
Step 200, mean loss 0.06397455469243586
Step 225, mean loss 0.07519298332189808
Unrolled forward losses 1.6297860379040707
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  10:58:15.997978 

Epoch 9
Starting epoch 9...
Training Loss (progress: 0.00): 0.1388394399516953; Norm Grads: 83.58296508824822
Training Loss (progress: 0.10): 0.1406666615972273; Norm Grads: 66.11126765566543
Training Loss (progress: 0.20): 0.13447137391135733; Norm Grads: 60.58764640004706
Training Loss (progress: 0.30): 0.13038421661559407; Norm Grads: 70.69915362614108
Training Loss (progress: 0.40): 0.13025947925586073; Norm Grads: 65.9435906629393
Training Loss (progress: 0.50): 0.1356619049104542; Norm Grads: 85.66448932138064
Training Loss (progress: 0.60): 0.13215601635935337; Norm Grads: 65.68419321015038
Training Loss (progress: 0.70): 0.1326428770043369; Norm Grads: 70.6784768279934
Training Loss (progress: 0.80): 0.13400685433502083; Norm Grads: 64.72283539127065
Training Loss (progress: 0.90): 0.13994276331640887; Norm Grads: 76.48918564579606
Evaluation on validation dataset:
Step 25, mean loss 0.031382473355032726
Step 50, mean loss 0.02030115710995071
Step 75, mean loss 0.02447976839419096
Step 100, mean loss 0.0252122669266383
Step 125, mean loss 0.03253379566082836
Step 150, mean loss 0.03966878370129285
Step 175, mean loss 0.08082681177567402
Step 200, mean loss 0.07033470161325661
Step 225, mean loss 0.07858397944964178
Unrolled forward losses 1.5432713350417904
Unrolled forward base losses 3.170855294869908
Epoch 10
Starting epoch 10...
Training Loss (progress: 0.00): 0.12429357128762175; Norm Grads: 54.75365414023074
Training Loss (progress: 0.10): 0.13508589349451916; Norm Grads: 60.71284252230349
Training Loss (progress: 0.20): 0.12261461283897654; Norm Grads: 57.80025735841915
Training Loss (progress: 0.30): 0.1208721543407945; Norm Grads: 68.9741752702786
Training Loss (progress: 0.40): 0.1307725609344446; Norm Grads: 64.38458988940823
Training Loss (progress: 0.50): 0.12697156655315878; Norm Grads: 55.46675093777279
Training Loss (progress: 0.60): 0.12278146525946833; Norm Grads: 62.42359533800835
Training Loss (progress: 0.70): 0.1205491152834804; Norm Grads: 56.576795277257624
Training Loss (progress: 0.80): 0.12381893697587615; Norm Grads: 61.61638060129268
Training Loss (progress: 0.90): 0.11884412938467609; Norm Grads: 62.63586752640603
Evaluation on validation dataset:
Step 25, mean loss 0.028843568295436522
Step 50, mean loss 0.018848276613320267
Step 75, mean loss 0.022602843166530332
Step 100, mean loss 0.024085397285889026
Step 125, mean loss 0.030223777037159276
Step 150, mean loss 0.03756398500305715
Step 175, mean loss 0.06925236148518904
Step 200, mean loss 0.06680242556758116
Step 225, mean loss 0.07437303904586995
Unrolled forward losses 1.3999644637717013
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.0248339668692781
Step 50, mean loss 0.01759966224903565
Step 75, mean loss 0.021509153257307016
Step 100, mean loss 0.022047184213485078
Step 125, mean loss 0.029147763702105093
Step 150, mean loss 0.03334061332946852
Step 175, mean loss 0.06659211290422676
Step 200, mean loss 0.056074452607280535
Step 225, mean loss 0.06962950560268621
Unrolled forward losses 1.5299538619773432
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  13:33:29.680811 

Epoch 11
Starting epoch 11...
Training Loss (progress: 0.00): 0.11711257518475927; Norm Grads: 51.256403338849225
Training Loss (progress: 0.10): 0.10365476687657174; Norm Grads: 56.59934627868987
Training Loss (progress: 0.20): 0.12752624868086604; Norm Grads: 59.25898827440731
Training Loss (progress: 0.30): 0.12766274508223682; Norm Grads: 59.88934085525555
Training Loss (progress: 0.40): 0.11822598154797723; Norm Grads: 57.51388632686011
Training Loss (progress: 0.50): 0.135457881582719; Norm Grads: 58.75774001618978
Training Loss (progress: 0.60): 0.11557424460125013; Norm Grads: 61.43560095763532
Training Loss (progress: 0.70): 0.11934684088531458; Norm Grads: 64.38855560922865
Training Loss (progress: 0.80): 0.11670791927793175; Norm Grads: 62.46055026296829
Training Loss (progress: 0.90): 0.12301371434142801; Norm Grads: 59.29332708579519
Evaluation on validation dataset:
Step 25, mean loss 0.027680008399392553
Step 50, mean loss 0.018514021262279762
Step 75, mean loss 0.02275792487194166
Step 100, mean loss 0.023551881370122273
Step 125, mean loss 0.029527293960283637
Step 150, mean loss 0.03690827050599137
Step 175, mean loss 0.07049432916327852
Step 200, mean loss 0.06454552910183008
Step 225, mean loss 0.07396837950257776
Unrolled forward losses 1.5015396115097945
Unrolled forward base losses 3.170855294869908
Epoch 12
Starting epoch 12...
Training Loss (progress: 0.00): 0.11706094338643623; Norm Grads: 57.128990932222735
Training Loss (progress: 0.10): 0.11380523253367329; Norm Grads: 69.87333636532011
Training Loss (progress: 0.20): 0.12096599383010402; Norm Grads: 57.0427472614133
Training Loss (progress: 0.30): 0.12816510940066772; Norm Grads: 60.86883310744112
Training Loss (progress: 0.40): 0.12585066505073542; Norm Grads: 63.35623454144761
Training Loss (progress: 0.50): 0.12833632996696515; Norm Grads: 66.18972123183275
Training Loss (progress: 0.60): 0.11422544436896027; Norm Grads: 59.17007702260002
Training Loss (progress: 0.70): 0.11777634787979949; Norm Grads: 56.86967379164591
Training Loss (progress: 0.80): 0.13444605103458004; Norm Grads: 59.28692890184801
Training Loss (progress: 0.90): 0.13059442844504135; Norm Grads: 61.24218981221309
Evaluation on validation dataset:
Step 25, mean loss 0.027351611499709463
Step 50, mean loss 0.018176677926411502
Step 75, mean loss 0.022176732018373446
Step 100, mean loss 0.02382794040038901
Step 125, mean loss 0.028887006778837692
Step 150, mean loss 0.036138111892861625
Step 175, mean loss 0.07831308319924865
Step 200, mean loss 0.06321439458103353
Step 225, mean loss 0.07113648809231682
Unrolled forward losses 1.4790701211350077
Unrolled forward base losses 3.170855294869908
Epoch 13
Starting epoch 13...
Training Loss (progress: 0.00): 0.12266983931916016; Norm Grads: 60.736116863749665
Training Loss (progress: 0.10): 0.12186665071087782; Norm Grads: 64.26747789375675
Training Loss (progress: 0.20): 0.1112716900492992; Norm Grads: 56.40562576110648
Training Loss (progress: 0.30): 0.12254379849254006; Norm Grads: 61.054285095559536
Training Loss (progress: 0.40): 0.12593121930897874; Norm Grads: 53.41848835370239
Training Loss (progress: 0.50): 0.11563154822927411; Norm Grads: 51.68015533476385
Training Loss (progress: 0.60): 0.12335733165427028; Norm Grads: 55.90965251195307
Training Loss (progress: 0.70): 0.11536363072231895; Norm Grads: 55.72042252630807
Training Loss (progress: 0.80): 0.10879418514353074; Norm Grads: 53.997775828588914
Training Loss (progress: 0.90): 0.1206852687699854; Norm Grads: 57.35038543975968
Evaluation on validation dataset:
Step 25, mean loss 0.02582928833790467
Step 50, mean loss 0.017058796208291833
Step 75, mean loss 0.021422327402212483
Step 100, mean loss 0.022337744407011178
Step 125, mean loss 0.028838990307525986
Step 150, mean loss 0.034747890226774235
Step 175, mean loss 0.07160425524234759
Step 200, mean loss 0.062083621206199854
Step 225, mean loss 0.07050881396865963
Unrolled forward losses 1.3218501083806071
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.02244366440616018
Step 50, mean loss 0.015651705922302972
Step 75, mean loss 0.019447555643982695
Step 100, mean loss 0.021309841014222113
Step 125, mean loss 0.027530631992170592
Step 150, mean loss 0.032337063264618826
Step 175, mean loss 0.06687800456527748
Step 200, mean loss 0.05499817879665788
Step 225, mean loss 0.06564319284286409
Unrolled forward losses 1.4157732281006663
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  17:21:12.813395 

Epoch 14
Starting epoch 14...
Training Loss (progress: 0.00): 0.12055473360696829; Norm Grads: 54.49728553037197
Training Loss (progress: 0.10): 0.12492724111969748; Norm Grads: 50.960051329550645
Training Loss (progress: 0.20): 0.11810605523151492; Norm Grads: 53.65459804105261
Training Loss (progress: 0.30): 0.1311576141780014; Norm Grads: 56.90262328460824
Training Loss (progress: 0.40): 0.11461986922187983; Norm Grads: 61.77849502143027
Training Loss (progress: 0.50): 0.11834998495687891; Norm Grads: 61.613874952740275
Training Loss (progress: 0.60): 0.1178861631289401; Norm Grads: 54.14122740115217
Training Loss (progress: 0.70): 0.1147885389566332; Norm Grads: 62.01421287975072
Training Loss (progress: 0.80): 0.12465754026468898; Norm Grads: 61.9091647364839
Training Loss (progress: 0.90): 0.11029570131014794; Norm Grads: 55.76360229195764
Evaluation on validation dataset:
Step 25, mean loss 0.02528941393923732
Step 50, mean loss 0.016186181111890928
Step 75, mean loss 0.021017517732184555
Step 100, mean loss 0.02215059204433649
Step 125, mean loss 0.028643609007680385
Step 150, mean loss 0.035152769672622
Step 175, mean loss 0.07209912658942422
Step 200, mean loss 0.06338952720690068
Step 225, mean loss 0.07100924786054236
Unrolled forward losses 1.360810557230461
Unrolled forward base losses 3.170855294869908
Epoch 15
Starting epoch 15...
Training Loss (progress: 0.00): 0.10822232650832374; Norm Grads: 47.72342379795704
Training Loss (progress: 0.10): 0.12251754605144007; Norm Grads: 54.68580140164807
Training Loss (progress: 0.20): 0.11969164935300615; Norm Grads: 50.653003823241875
Training Loss (progress: 0.30): 0.11972095715640235; Norm Grads: 54.16869665114172
Training Loss (progress: 0.40): 0.11960880252687224; Norm Grads: 49.698648747832515
Training Loss (progress: 0.50): 0.11721634209997953; Norm Grads: 48.10918197937088
Training Loss (progress: 0.60): 0.11860775037571236; Norm Grads: 54.932252049681864
Training Loss (progress: 0.70): 0.1166992009739248; Norm Grads: 47.5291591094276
Training Loss (progress: 0.80): 0.11416672496268285; Norm Grads: 59.566975948704616
Training Loss (progress: 0.90): 0.11442170920095318; Norm Grads: 48.50135629363019
Evaluation on validation dataset:
Step 25, mean loss 0.024541604931568668
Step 50, mean loss 0.01629424498651394
Step 75, mean loss 0.020339667588393215
Step 100, mean loss 0.021349456612838658
Step 125, mean loss 0.02724075701253474
Step 150, mean loss 0.03374710519562728
Step 175, mean loss 0.0707915749724419
Step 200, mean loss 0.060595012479600044
Step 225, mean loss 0.0665215329182939
Unrolled forward losses 1.3483776159499339
Unrolled forward base losses 3.170855294869908
Epoch 16
Starting epoch 16...
Training Loss (progress: 0.00): 0.10784203658542714; Norm Grads: 47.60037108618942
Training Loss (progress: 0.10): 0.10460367418810504; Norm Grads: 49.40011191173834
Training Loss (progress: 0.20): 0.11584374549818238; Norm Grads: 50.28957730660697
Training Loss (progress: 0.30): 0.12213326831279982; Norm Grads: 58.67473955928675
Training Loss (progress: 0.40): 0.10426283651643799; Norm Grads: 49.967304260184484
Training Loss (progress: 0.50): 0.1046963333963513; Norm Grads: 52.27590036379544
Training Loss (progress: 0.60): 0.1101802690238117; Norm Grads: 54.21512775954611
Training Loss (progress: 0.70): 0.11698696885531204; Norm Grads: 57.97830181027364
Training Loss (progress: 0.80): 0.11095836714340225; Norm Grads: 48.96813910153255
Training Loss (progress: 0.90): 0.11392156082222593; Norm Grads: 50.71295777879611
Evaluation on validation dataset:
Step 25, mean loss 0.024021382601897123
Step 50, mean loss 0.015502509275950094
Step 75, mean loss 0.020230365598957578
Step 100, mean loss 0.02139594885288107
Step 125, mean loss 0.02653395664076498
Step 150, mean loss 0.033593193453720735
Step 175, mean loss 0.07176543242084359
Step 200, mean loss 0.05917338149295373
Step 225, mean loss 0.06596495502623348
Unrolled forward losses 1.3248874819941938
Unrolled forward base losses 3.170855294869908
Epoch 17
Starting epoch 17...
Training Loss (progress: 0.00): 0.11689218299880821; Norm Grads: 46.34809726854594
Training Loss (progress: 0.10): 0.11187884435166327; Norm Grads: 57.49074923917107
Training Loss (progress: 0.20): 0.11591263565487069; Norm Grads: 51.15684763322218
Training Loss (progress: 0.30): 0.10455443475125552; Norm Grads: 57.113855011011616
Training Loss (progress: 0.40): 0.11818413242595965; Norm Grads: 54.44375797250921
Training Loss (progress: 0.50): 0.11349170744008188; Norm Grads: 58.64030113613615
Training Loss (progress: 0.60): 0.11592639366781841; Norm Grads: 57.594534165996016
Training Loss (progress: 0.70): 0.1152861448536209; Norm Grads: 51.64986213071096
Training Loss (progress: 0.80): 0.09993718505956486; Norm Grads: 53.360126053144754
Training Loss (progress: 0.90): 0.10311095340396208; Norm Grads: 46.6395088333232
Evaluation on validation dataset:
Step 25, mean loss 0.023724690312179476
Step 50, mean loss 0.015244453299004212
Step 75, mean loss 0.019679547412216267
Step 100, mean loss 0.02048768503108142
Step 125, mean loss 0.026315428300897874
Step 150, mean loss 0.03330050094706299
Step 175, mean loss 0.0710193331589037
Step 200, mean loss 0.0592755437967282
Step 225, mean loss 0.06524974408563258
Unrolled forward losses 1.271715500599715
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.020052525211377004
Step 50, mean loss 0.014621247328700687
Step 75, mean loss 0.018315512808209193
Step 100, mean loss 0.019335625712874796
Step 125, mean loss 0.02537006061834598
Step 150, mean loss 0.029784581456631615
Step 175, mean loss 0.06161701278474098
Step 200, mean loss 0.05127819192337447
Step 225, mean loss 0.06198655747839346
Unrolled forward losses 1.3405739064090132
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  22:13:14.809505 

Epoch 18
Starting epoch 18...
Training Loss (progress: 0.00): 0.11082540120742884; Norm Grads: 50.39802900966446
Training Loss (progress: 0.10): 0.1155011514695588; Norm Grads: 56.575886455071824
Training Loss (progress: 0.20): 0.11750577333526932; Norm Grads: 51.54051982074558
Training Loss (progress: 0.30): 0.11493746812338348; Norm Grads: 55.51269730764534
Training Loss (progress: 0.40): 0.11600579087405416; Norm Grads: 48.04476324755437
Training Loss (progress: 0.50): 0.12381092574576426; Norm Grads: 53.03179358802104
Training Loss (progress: 0.60): 0.11106172658021625; Norm Grads: 49.041146398072044
Training Loss (progress: 0.70): 0.11019857492811366; Norm Grads: 51.28978639933913
Training Loss (progress: 0.80): 0.11227870591457137; Norm Grads: 55.103681686456795
Training Loss (progress: 0.90): 0.12344966664006166; Norm Grads: 54.97279737876698
Evaluation on validation dataset:
Step 25, mean loss 0.022928898238239265
Step 50, mean loss 0.015435435187158347
Step 75, mean loss 0.019712211692324497
Step 100, mean loss 0.020715009087934153
Step 125, mean loss 0.026219962565160557
Step 150, mean loss 0.032443487805179354
Step 175, mean loss 0.07123003965805058
Step 200, mean loss 0.058642008826500204
Step 225, mean loss 0.06515540034578356
Unrolled forward losses 1.2849844434085074
Unrolled forward base losses 3.170855294869908
Epoch 19
Starting epoch 19...
Training Loss (progress: 0.00): 0.1132059202452361; Norm Grads: 53.567815246403825
Training Loss (progress: 0.10): 0.11280483713990931; Norm Grads: 49.851897712135035
Training Loss (progress: 0.20): 0.10564274703702192; Norm Grads: 57.68485408272584
Training Loss (progress: 0.30): 0.1174065693700644; Norm Grads: 48.904808710912334
Training Loss (progress: 0.40): 0.10733301525513701; Norm Grads: 50.65286158024758
Training Loss (progress: 0.50): 0.10977928060469236; Norm Grads: 50.107242511613
Training Loss (progress: 0.60): 0.10719427285195339; Norm Grads: 49.73207132900855
Training Loss (progress: 0.70): 0.11433433601382036; Norm Grads: 49.58217403420028
Training Loss (progress: 0.80): 0.10569526360893963; Norm Grads: 53.29138545616965
Training Loss (progress: 0.90): 0.10773770512203475; Norm Grads: 53.388718669216175
Evaluation on validation dataset:
Step 25, mean loss 0.022895084199844522
Step 50, mean loss 0.015246581782667384
Step 75, mean loss 0.019426463408312767
Step 100, mean loss 0.02093532362092551
Step 125, mean loss 0.026082953949675393
Step 150, mean loss 0.03263631589063162
Step 175, mean loss 0.0709291009885003
Step 200, mean loss 0.05913528478611734
Step 225, mean loss 0.06447763005665816
Unrolled forward losses 1.2606725119509836
Unrolled forward base losses 3.170855294869908
Evaluation on test dataset:
Step 25, mean loss 0.01956155674201321
Step 50, mean loss 0.014819440469401838
Step 75, mean loss 0.018456025311638626
Step 100, mean loss 0.018821972488850182
Step 125, mean loss 0.02513650353462791
Step 150, mean loss 0.02874642751502531
Step 175, mean loss 0.06017625387004068
Step 200, mean loss 0.05050991311157892
Step 225, mean loss 0.060488644662845814
Unrolled forward losses 1.3441370402737336
Unrolled forward base losses 3.233795614931353
Saved model at models/GNN_CE_E1_xresolution100-200_n3_tw25_unrolling2_time3102142_randomregdeg6_alternating.pt
Training time:  1 day, 0:33:12.147140 

Epoch 20
Starting epoch 20...
Training Loss (progress: 0.00): 0.10416724105351162; Norm Grads: 61.86728353108065
Training Loss (progress: 0.10): 0.11873496577638006; Norm Grads: 45.00246642720982
Training Loss (progress: 0.20): 0.10461054688435027; Norm Grads: 47.12354858059697
Training Loss (progress: 0.30): 0.1290925277221484; Norm Grads: 49.20486374011593
Training Loss (progress: 0.40): 0.1132596776446058; Norm Grads: 45.92767261951737
Training Loss (progress: 0.50): 0.10153460153565348; Norm Grads: 51.85200960718358
Training Loss (progress: 0.60): 0.10813878042779093; Norm Grads: 51.525797464124615
Training Loss (progress: 0.70): 0.10485197670116077; Norm Grads: 48.52772051065969
Training Loss (progress: 0.80): 0.11152462978365993; Norm Grads: 54.90314221537703
Training Loss (progress: 0.90): 0.11394580986498878; Norm Grads: 51.45123266259818
Evaluation on validation dataset:
Step 25, mean loss 0.02279716735231567
Step 50, mean loss 0.015168366949632101
Step 75, mean loss 0.019196133820355257
Step 100, mean loss 0.020404168619151432
Step 125, mean loss 0.025771428397889998
Step 150, mean loss 0.03249675323351735
Step 175, mean loss 0.0690021464656975
Step 200, mean loss 0.05796918757545145
Step 225, mean loss 0.06391569023168153
Unrolled forward losses 1.284207576700437
Unrolled forward base losses 3.170855294869908
Epoch 21
Starting epoch 21...
Training Loss (progress: 0.00): 0.11424500225078081; Norm Grads: 48.146467185275185
Training Loss (progress: 0.10): 0.11135164335619181; Norm Grads: 54.97198643940197
Training Loss (progress: 0.20): 0.10837734998347796; Norm Grads: 50.078972829336486
Training Loss (progress: 0.30): 0.10989347705943912; Norm Grads: 48.96465565525348
Training Loss (progress: 0.40): 0.09920327727386029; Norm Grads: 52.70730399135222
Training Loss (progress: 0.50): 0.11668355053438116; Norm Grads: 50.33526089657045
Training Loss (progress: 0.60): 0.11239999295074013; Norm Grads: 58.159882543851516
Training Loss (progress: 0.70): 0.10520004370893958; Norm Grads: 58.62198537890619
Training Loss (progress: 0.80): 0.11257336326474189; Norm Grads: 52.50786274728494
Training Loss (progress: 0.90): 0.10994843470961241; Norm Grads: 51.51061665063628
Evaluation on validation dataset:
Step 25, mean loss 0.022496814441541658
Step 50, mean loss 0.014546111244514876
Step 75, mean loss 0.019256017631561404
Step 100, mean loss 0.02022047725878026
Step 125, mean loss 0.026142148947735963
Step 150, mean loss 0.03263463123249437
Step 175, mean loss 0.06924670892794099
Step 200, mean loss 0.057148058380833304
Step 225, mean loss 0.06493774002568473
Unrolled forward losses 1.2816844146454676
Unrolled forward base losses 3.170855294869908
Epoch 22
Starting epoch 22...
Training Loss (progress: 0.00): 0.09931755915795808; Norm Grads: 52.63511507466173
Training Loss (progress: 0.10): 0.11661557780594108; Norm Grads: 58.58181041943417
Training Loss (progress: 0.20): 0.11217115945171319; Norm Grads: 50.60427706195772
Training Loss (progress: 0.30): 0.10149319901886093; Norm Grads: 51.49574393774996
Training Loss (progress: 0.40): 0.10554027612599595; Norm Grads: 51.610162965243816
Training Loss (progress: 0.50): 0.09917372074232611; Norm Grads: 57.19542370189375
Training Loss (progress: 0.60): 0.10346691386730235; Norm Grads: 50.436412049633574
Training Loss (progress: 0.70): 0.10949735047259927; Norm Grads: 48.69102665744203
Training Loss (progress: 0.80): 0.10068849336753792; Norm Grads: 47.70167578757303
Training Loss (progress: 0.90): 0.10615540416037808; Norm Grads: 49.2069103699041
Evaluation on validation dataset:
Step 25, mean loss 0.022266204441943188
Step 50, mean loss 0.014601248772705685
Step 75, mean loss 0.018899428728657297
Step 100, mean loss 0.020247384450377166
Step 125, mean loss 0.025546102586512992
Step 150, mean loss 0.031814488288124665
Step 175, mean loss 0.06927788545671312
Step 200, mean loss 0.059128820645918734
Step 225, mean loss 0.0632755673611871
Unrolled forward losses 1.2712040384292247
Unrolled forward base losses 3.170855294869908
Epoch 23
Starting epoch 23...
Training Loss (progress: 0.00): 0.10324717789744137; Norm Grads: 50.391371730584474
Training Loss (progress: 0.10): 0.12456168727194; Norm Grads: 50.049136887895976
Training Loss (progress: 0.20): 0.12074506214218352; Norm Grads: 51.16116162113245
Training Loss (progress: 0.30): 0.09980979944570978; Norm Grads: 55.857387417468075
Training Loss (progress: 0.40): 0.10470659511192751; Norm Grads: 57.79367941210724
Training Loss (progress: 0.50): 0.10764408117600872; Norm Grads: 51.69927277569333
Training Loss (progress: 0.60): 0.11048796906436997; Norm Grads: 49.9198819652209
Training Loss (progress: 0.70): 0.12043381599009353; Norm Grads: 49.19793824021032
Training Loss (progress: 0.80): 0.11328953873417671; Norm Grads: 56.75921565376477
Training Loss (progress: 0.90): 0.11146411065745608; Norm Grads: 49.83977987388393
Evaluation on validation dataset:
Step 25, mean loss 0.021321375904001107
Step 50, mean loss 0.014503496514750344
Step 75, mean loss 0.019039299890927834
Step 100, mean loss 0.02021286937907177
Step 125, mean loss 0.025268842966011017
Step 150, mean loss 0.03219547907559822
Step 175, mean loss 0.06687973773299524
Step 200, mean loss 0.05720830295184757
Step 225, mean loss 0.06318463983076907
Unrolled forward losses 1.2734578494665156
Unrolled forward base losses 3.170855294869908
Epoch 24
Starting epoch 24...
Training Loss (progress: 0.00): 0.10243354499164165; Norm Grads: 44.95946614774996
Training Loss (progress: 0.10): 0.10976219123227288; Norm Grads: 49.96824692380984
Training Loss (progress: 0.20): 0.09598376109652998; Norm Grads: 49.29352879285361
Training Loss (progress: 0.30): 0.10133358719606679; Norm Grads: 57.02401164198472
Training Loss (progress: 0.40): 0.11325831645706422; Norm Grads: 51.55525985047867
Training Loss (progress: 0.50): 0.10743656541678512; Norm Grads: 48.5499245425633
Training Loss (progress: 0.60): 0.11176539013837623; Norm Grads: 47.84408965870883
Training Loss (progress: 0.70): 0.10856386612740397; Norm Grads: 50.07260352553876
Training Loss (progress: 0.80): 0.11390040073972442; Norm Grads: 52.98948425279382
Training Loss (progress: 0.90): 0.10464804545806965; Norm Grads: 51.737094646566476
Evaluation on validation dataset:
Step 25, mean loss 0.02139845288491097
Step 50, mean loss 0.01444794976198077
Step 75, mean loss 0.019300235336055818
Step 100, mean loss 0.02025287333426328
Step 125, mean loss 0.024932042482811015
Step 150, mean loss 0.031688281238491156
Step 175, mean loss 0.07147293555001186
Step 200, mean loss 0.05780292054989402
Step 225, mean loss 0.06194310329843805
Unrolled forward losses 1.2850177787920094
Unrolled forward base losses 3.170855294869908
Test loss: 1.3441370402737336
Training time (until epoch 19):  {datetime.timedelta(days=1, seconds=1992, microseconds=147140)}
